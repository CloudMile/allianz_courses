{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2_cont.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "6ZkmV_Ly4QfR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting started: Introduction to Keras\n",
        "\n",
        "The core data structure of Keras is a __model__, a way to organize layers. The simplest type of model is the [`Sequential`](https://keras.io/getting-started/sequential-model-guide) model, a linear stack of layers. For more complex architectures, you should use the [Keras functional API](https://keras.io/getting-started/functional-api-guide), which allows to build arbitrary graphs of layers.\n",
        "\n",
        "\n",
        "Here is the `Sequential` model:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "```\n",
        "\n",
        "Stacking layers is as easy as `.add()`:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "```\n",
        "\n",
        "Once your model looks good, configure its learning process with `.compile()`:\n",
        "\n",
        "```python\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).\n",
        "```python\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
        "```\n",
        "\n",
        "You can now iterate on your training data in batches:\n",
        "\n",
        "```python\n",
        "# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "```\n",
        "\n",
        "Evaluate your performance in one line:\n",
        "\n",
        "```python\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
        "```\n",
        "\n",
        "Or generate predictions on new data:\n",
        "\n",
        "```python\n",
        "classes = model.predict(x_test, batch_size=128)\n",
        "```\n",
        "\n",
        "Building a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?\n"
      ]
    },
    {
      "metadata": {
        "id": "9OWIA1k1U2T9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Necessary Library"
      ]
    },
    {
      "metadata": {
        "id": "MZL_r7cGHUxF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "me6WovuPek-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos]\n",
        "\n",
        "get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "geI8IW1BiLxL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(2017)\n",
        "tf.set_random_seed(2017)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4MMl5E8TUzRD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "metadata": {
        "id": "xkFZh347Hdwf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wx3xrroK8YcY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "metadata": {
        "id": "a3dV0Q9ur6cK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YIS_EzSz9ZRH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://storage.googleapis.com/allianz-course/data/fashion_mnist_label.jpg =200x400)"
      ]
    },
    {
      "metadata": {
        "id": "ygSkiSTyHi5k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EBppeucZ8gz4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preview some images in each class\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(4):\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    \n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(str(train_labels[i])+': '+class_names[train_labels[i]], size=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGaphirWHgMa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Train image shape:{0}\".format(train_images.shape))\n",
        "print(\"Test image shape:{0}\".format(test_images.shape))\n",
        "print(\"Train class: {0}\".format(np.unique(train_labels)))\n",
        "print(\"Test class: {0}\".format(np.unique(test_labels)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RwoktLzynhwR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n"
      ]
    },
    {
      "metadata": {
        "id": "-8E3fGplpZyc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Normalizing(feature_scaling)\n",
        "\n",
        "\n",
        "In machine learning, we want the model to be able to learn the real structures instead of dealing with the scales difference. Therefore, we would normalize data before feeding it into model.\n",
        "\n",
        "![Normalizing(feature_scaling)](https://storage.googleapis.com/allianz-course/data/feature_scaling.jpg =300x150)"
      ]
    },
    {
      "metadata": {
        "id": "O6FJqsbKCZck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "print('Max value in this image: {}'.format(np.amax(train_images[0])))\n",
        "print('Min value in this image: {}'.format(np.amin(train_images[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "836Bwgl1n9Ml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Normalize Data\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PxpbZqwIeyq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **One-hot encoding**\n",
        "\n",
        "The label now is 0, 1, 2, 3, ...., 9 and they are 'symbols' of classes. However, if we use 0,1,2,3,...,9 to indicate classes, there would be ordinal relationship between the classes.  Therefore, one-hot encoding method is applied to label before training.\n",
        "\n",
        "![alt text](https://storage.googleapis.com/allianz-course/data/one-hot.jpg =600x400)"
      ]
    },
    {
      "metadata": {
        "id": "epQZqJejnXwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_labels = keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "test_labels = keras.utils.to_categorical(test_labels, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "smB9CUPMBS5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lVOavh0mTyJz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ]
    },
    {
      "metadata": {
        "id": "5eaZIcm5Qdpe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **(Lab 2-a) Basic Model: model_sig_sgd_001**\n",
        "* Hidden Layer: [ 128 , 64 ]\n",
        "* Activation funcition in Hidden Layers: Sigmoid\n",
        "* Optimizer: SGD\n",
        "* Learning Rate: 0.001\n",
        "* Training Epoch: 20\n"
      ]
    },
    {
      "metadata": {
        "id": "yv6Z5OVTTcyV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **(Lab 2-b) Change activation function to Relu: model_relu_sgd_001**\n",
        "\n",
        "Now, change the activation function in hidden layers to RELU with the following details:\n",
        "\n",
        "* Hidden Layer: [ 128 , 64 ]\n",
        "* Activation funcition in Hidden Layers: **Relu**\n",
        "* Optimizer: SGD\n",
        "* Learning Rate: 0.001\n",
        "* Training Epoch: 20"
      ]
    },
    {
      "metadata": {
        "id": "ESR7KeLJXkUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (Lab 2-c) Change optimizer to Adam: model_relu_adam_001\n",
        "\n",
        "Now, change the optimizer to Adam with the following details:\n",
        "\n",
        "* Hidden Layer: [ 128 , 64 ]\n",
        "* Activation funcition in Hidden Layers: Relu\n",
        "* Optimizer: **Adam**\n",
        "* Learning Rate: 0.001\n",
        "* Training Epoch: 20\n",
        "\n",
        "***Hint***: \n",
        "\n",
        "To use adam optimizer:\n",
        "```python\n",
        "# learning rate is lr\n",
        "opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "IYoLSwn-Xi83",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Clean session fisrt\n",
        "K.clear_session()\n",
        "\n",
        "# Start building the model \n",
        "model_relu_adam_001 = Sequential()\n",
        "model_relu_adam_001.add(Flatten(input_shape=(28, 28)))\n",
        "model_relu_adam_001.add(Dense(128, activation = 'relu'))\n",
        "model_relu_adam_001.add(Dense(64, activation = 'relu'))\n",
        "model_relu_adam_001.add(Dense(10, activation='softmax'))\n",
        "\n",
        "############# START CODING HERE #############\n",
        "\n",
        "# create a adam optimizer with learning rate 0.001 (~ 1 line)(hint: tf.train.AdamOptimizer(learning_rate = n))\n",
        "opt = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
        "\n",
        "############# END CODING HERE ###############\n",
        "\n",
        "model_relu_adam_001.compile(loss='categorical_crossentropy',\n",
        "                           optimizer = opt,\n",
        "                           metrics = ['accuracy'])\n",
        "\n",
        "# Use .summary() to see model details\n",
        "model_relu_adam_001.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQDEvh7Yel-b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_relu_adam_001 = model_relu_adam_001.fit(train_images, \n",
        "                                              train_labels, \n",
        "                                              epochs = 20, \n",
        "                                              batch_size = 128, \n",
        "                                              validation_split = 0.05, \n",
        "                                              shuffle = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LyUfnqPMzyIW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "res = model_relu_adam_001.evaluate(test_images, test_labels)\n",
        "print(f'Testing Accuracy of model_relu_adam_001: {res[1]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VDh4qG3Nem09",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###(Lab 2-d) Change Learning Rate to 0.01: model_relu_adam_01\n",
        "\n",
        "Now, change the learning rate to 0.01 with the following details:\n",
        "\n",
        "* Hidden Layer: [ 128 , 64 ]\n",
        "* Activation funcition in Hidden Layers: Relu\n",
        "* Optimizer: Adam\n",
        "* Learning Rate: **0.01**\n",
        "* Training Epoch: 20\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gEx5R6sCTl1h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (Lab 2-e) Add more neurons in hidden layers(overfit) : model_large_relu_adam_001\n",
        "\n",
        "Now, change hidden layer neurons to [2048, 1024] with the following details:\n",
        "\n",
        "* Hidden Layer: [ 2048 , 1024 ]\n",
        "* Activation funcition in Hidden Layers: **Relu**\n",
        "* Optimizer: Adam\n",
        "* Learning Rate: 0.001\n",
        "* Training Epoch: 20\n",
        "\n",
        "***Hint***: \n",
        "\n",
        "To use adam optimizer with learning rate lr:\n",
        "```python\n",
        "# learning rate is lr\n",
        "opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "M2Y5lPvJDWzD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Clean session fisrt\n",
        "K.clear_session()\n",
        "\n",
        "# Start building the model \n",
        "model_large_relu_adam_001 = Sequential()\n",
        "model_large_relu_adam_001.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "############# START CODING HERE #############\n",
        "\n",
        "# Create hidden layer with 2048 neurons and relu activation function (~ 1 line)\n",
        "model_large_relu_adam_001.add(Dense(2048, activation = 'relu'))\n",
        "\n",
        "\n",
        "# Create hidden layer with 1024 neurons and relu activation function (~ 1 line)\n",
        "model_large_relu_adam_001.add(Dense(1024, activation = 'relu'))\n",
        "\n",
        "# Create an output layer with 10 neurons and softmax activation function (~ 1 line)\n",
        "model_large_relu_adam_001.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "\n",
        "# Create a Adam optimizer with learning rate = 0.001 ( ~ 1 line)(hint: tf.train.AdamOptimizer(learning_rate = n))\n",
        "opt = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
        "\n",
        "\n",
        "############# END CODING HERE ###############\n",
        "\n",
        "\n",
        "\n",
        "model_large_relu_adam_001.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer = opt,\n",
        "                                  metrics = ['accuracy'])\n",
        "\n",
        "# Use .summary() to see model details\n",
        "model_large_relu_adam_001.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AdUe2F9DetYa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_large_relu_adam_001 = model_large_relu_adam_001.fit(train_images, \n",
        "                                                          train_labels, \n",
        "                                                          epochs = 20, \n",
        "                                                          batch_size = 128, \n",
        "                                                          validation_split = 0.05, \n",
        "                                                          shuffle = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GFJpWawyDadC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "epoch = len(train_large_relu_adam_001.history[\"loss\"])\n",
        "plt.plot(np.arange(1, epoch+1), train_large_relu_adam_001.history['loss'], label='Train', lw=3)\n",
        "plt.plot(np.arange(1, epoch+1), train_large_relu_adam_001.history['val_loss'], label='Validation', lw=3)\n",
        "plt.ylabel('loss', family='serif', size=14)\n",
        "plt.xlabel('Epoch #', family='serif', size=14)\n",
        "plt.xticks(np.arange(1, epoch+1))\n",
        "plt.xlim([1, epoch])\n",
        "plt.legend(prop={'size':14, 'family':'serif'})\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X_hnjjUDaSUj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Overfit Solution**\n"
      ]
    },
    {
      "metadata": {
        "id": "WE2q3d1sVRVg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### (Lab 2-f) Early Stopping\n",
        "\n",
        "Stop training when a monitored quantity has stopped improving.\n",
        "\n",
        "```python\n",
        "\n",
        "keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, mode='auto')\n",
        "\n",
        "```\n",
        "* monitor: quantity to be monitored.\n",
        "\n",
        "* patience: number of epochs with no improvement after which training will be stopped.\n",
        "\n",
        "* mode: one of {auto, min, max}.    \n",
        "<br/>\n"
      ]
    },
    {
      "metadata": {
        "id": "BIICxJ7-VhrR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (Lab 2-g) Dropout\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://storage.googleapis.com/allianz-course/data/dropout.jpg =400x200)\n",
        "\n",
        "<br/>\n",
        "Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.\n",
        "```python\n",
        "keras.layers.Dropout(rate)\n",
        "```\n",
        "rate: float between 0 and 1. Fraction of the input units to drop.\n"
      ]
    },
    {
      "metadata": {
        "id": "YUuFFwZZNhtn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (Lab 2-h) Regularization"
      ]
    },
    {
      "metadata": {
        "id": "MpAa-z83SvbR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HWkHRCQ8N7v6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Clean session fisrt\n",
        "K.clear_session()\n",
        "\n",
        "# Start building the model \n",
        "model_large_relu_adam_001_re = Sequential()\n",
        "\n",
        "model_large_relu_adam_001_re.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "model_large_relu_adam_001_re.add(Dense(2048, \n",
        "                                       activation = 'relu', \n",
        "                                       kernel_regularizer=regularizers.l2(0.001)))\n",
        "\n",
        "############# START CODING HERE ############\n",
        "\n",
        "# Add layer Dense layer with 1024 neurons, activation funtion = 'relu' and \n",
        "# l2 regularizer with lambda = 0.001\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############# END CODING HERE #############\n",
        "\n",
        "\n",
        "model_large_relu_adam_001_re.add(Dropout(0.5))\n",
        "\n",
        "model_large_relu_adam_001_re.add(Dense(10, \n",
        "                                       activation='softmax'))\n",
        "\n",
        "opt = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
        "\n",
        "model_large_relu_adam_001_re.compile(loss='categorical_crossentropy',\n",
        "                                     optimizer = opt,\n",
        "                                     metrics = ['accuracy'])\n",
        "\n",
        "# Use .summary() to see model details\n",
        "model_large_relu_adam_001_re.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XFMpkitI3Rw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Solution"
      ]
    },
    {
      "metadata": {
        "id": "1Nlgc5wPI35o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model_large_relu_adam_001_re.add(Dense(1024, \n",
        "#                                        activation = 'relu', \n",
        "#                                        kernel_regularizer=regularizers.l2(0.001)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCmUfC8MTEfM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_model_large_relu_adam_001_re = model_large_relu_adam_001_re.fit(train_images,\n",
        "                                                                      train_labels, \n",
        "                                                                      epochs=20, \n",
        "                                                                      validation_split = 0.05,\n",
        "                                                                      batch_size = 128,\n",
        "                                                                      shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_6VGVSnUSZxn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "epoch = len(train_model_large_relu_adam_001_re.history[\"loss\"])\n",
        "plt.plot(np.arange(1, epoch+1), train_model_large_relu_adam_001_re.history['loss'], label='Train', lw=3)\n",
        "plt.plot(np.arange(1, epoch+1), train_model_large_relu_adam_001_re.history['val_loss'], label='Validation', lw=3)\n",
        "plt.ylabel('loss', family='serif', size=14)\n",
        "plt.xlabel('Epoch #', family='serif', size=14)\n",
        "plt.xticks(np.arange(1, epoch+1))\n",
        "plt.xlim([1, epoch])\n",
        "plt.legend(prop={'size':14, 'family':'serif'})\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rAfEW1mWdixY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "CXeT3-FCSiq_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, BatchNormalization, Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2mYvKZgOpr-b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (Lab 2-i) Create a basic CNN model"
      ]
    },
    {
      "metadata": {
        "id": "R2EoVGQGSPe9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "model_basic_cnn = keras.Sequential()\n",
        "\n",
        "model_basic_cnn.add(Conv2D(filters=32,\n",
        "                           kernel_size=(3,3),\n",
        "                           input_shape=(28,28,1),\n",
        "                           padding='same',\n",
        "                           activation='relu'))\n",
        "\n",
        "model_basic_cnn.add(MaxPool2D(pool_size=(2,2),\n",
        "                              strides=(2,2)))\n",
        "\n",
        "model_basic_cnn.add(Conv2D(filters=64,\n",
        "                           kernel_size=(3,3),\n",
        "                           padding='same',\n",
        "                           activation='relu'))\n",
        "\n",
        "model_basic_cnn.add(MaxPool2D(pool_size=(2,2),\n",
        "                              strides=(2,2)))\n",
        "\n",
        "############# START CODING HERE #############\n",
        "\n",
        "# Create a Convolution 2D layer with 128 filters, \n",
        "# kernel size =(3,3), padding = 'same', activation = 'relu'(~ 1 line)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a 2D Max pooling layer with pooling size = (2,2) and strides = (2,2) (~ 1 line)\n",
        "\n",
        "\n",
        "\n",
        "############# END CODING HERE #############\n",
        "\n",
        "\n",
        "model_basic_cnn.add(Flatten())\n",
        "\n",
        "model_basic_cnn.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model_basic_cnn.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wGXJwPQM7Wtz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Solution"
      ]
    },
    {
      "metadata": {
        "id": "XDYugrlw7c-o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model_basic_cnn.add(Conv2D(filters=128,\n",
        "#                            kernel_size=(3,3),\n",
        "#                            padding='same',\n",
        "#                            activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "# model_basic_cnn.add(MaxPool2D(pool_size=(2,2),\n",
        "#                               strides=(2,2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLX_MkIBhdXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opfunc = tf.train.AdamOptimizer(learning_rate = 0.001) \n",
        "\n",
        "model_basic_cnn.compile(optimizer = opfunc, \n",
        "                        loss = 'categorical_crossentropy',\n",
        "                        metrics = ['accuracy'])\n",
        "\n",
        "train_basic_cnn = model_basic_cnn.fit(np.expand_dims(train_images, -1), \n",
        "                                      train_labels, \n",
        "                                      batch_size=256,\n",
        "                                      epochs=20, \n",
        "                                      validation_split = 0.05,\n",
        "                                      shuffle = False,\n",
        "                                      verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p7fVF1yAiMHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model_basic_cnn = model_basic_cnn.evaluate(np.expand_dims(test_images, -1) , test_labels)\n",
        "print(f'Testing Accuracy of the Basic CNN: {test_model_basic_cnn[1]}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hqwzJlmRxf9L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (Lab 2-j) Create a advanced CNN model"
      ]
    },
    {
      "metadata": {
        "id": "mt5oVlm-WQLC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "model_cnn = keras.Sequential()\n",
        "\n",
        "model_cnn.add(Conv2D(filters=32,\n",
        "                     kernel_size=(3,3),\n",
        "                     input_shape=(28,28,1),\n",
        "                     padding='valid',\n",
        "                     use_bias=False,\n",
        "                     activation=None))\n",
        "\n",
        "model_cnn.add(BatchNormalization())\n",
        "\n",
        "model_cnn.add(Activation('relu'))\n",
        "\n",
        "model_cnn.add(Conv2D(filters=64,\n",
        "                     kernel_size=(3,3),\n",
        "                     padding='valid',\n",
        "                     use_bias=False,\n",
        "                     activation=None))\n",
        "\n",
        "model_cnn.add(BatchNormalization())\n",
        "\n",
        "model_cnn.add(Activation('relu'))\n",
        "\n",
        "############# START CODING HERE #############\n",
        "\n",
        "# Create a Convolution 2D layer with 128 filters, \n",
        "# kernel size =(3,3), padding = 'same', not using bias and no activation(~ 1 line)\n",
        "\n",
        "\n",
        "\n",
        "# Add Batch Normalization (~ 1 line)\n",
        "\n",
        "\n",
        "# Add activation = relu  (~ 1 line)\n",
        "\n",
        "\n",
        "# Create a 2D Max pooling layer with pooling size = (2,2) and strides = (2,2) (~ 1 line)\n",
        "\n",
        "\n",
        "############# END CODING HERE #############\n",
        "\n",
        "model_cnn.add(Conv2D(filters=256,\n",
        "                     kernel_size=(3,3),\n",
        "                     padding='valid',\n",
        "                     use_bias=False,\n",
        "                     activation=None))\n",
        "\n",
        "model_cnn.add(BatchNormalization())  \n",
        "\n",
        "model_cnn.add(Activation('relu'))\n",
        "\n",
        "model_cnn.add(Conv2D(filters=512,\n",
        "                     kernel_size=(3,3),\n",
        "                     padding='valid',\n",
        "                     use_bias=False,\n",
        "                     activation=None))\n",
        "\n",
        "model_cnn.add(BatchNormalization())\n",
        "\n",
        "model_cnn.add(Activation('relu'))\n",
        "\n",
        "model_cnn.add(MaxPool2D(pool_size=(8,8),\n",
        "                        strides=(1,1)))  \n",
        "\n",
        "model_cnn.add(Flatten()) \n",
        "\n",
        "model_cnn.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model_cnn.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cofoC7Rp8PDa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Solution"
      ]
    },
    {
      "metadata": {
        "id": "yZsrM82e8KkV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model_cnn.add(Conv2D(filters=128,\n",
        "#                      kernel_size=(3,3),\n",
        "#                      padding='same',\n",
        "#                      use_bias=False,\n",
        "#                      activation=None))\n",
        "\n",
        "# model_cnn.add(BatchNormalization())  \n",
        "\n",
        "# model_cnn.add(Activation('relu'))\n",
        "\n",
        "# model_cnn.add(MaxPool2D(pool_size=(2,2),\n",
        "#                         strides=(2,2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cbNO3oSofz7F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opfunc = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
        "\n",
        "model_cnn.compile(optimizer = opfunc, \n",
        "                  loss = 'categorical_crossentropy',\n",
        "                  metrics = ['accuracy'])\n",
        "\n",
        "train_model_cnn = model_cnn.fit(np.expand_dims(train_images, -1),\n",
        "                                train_labels, \n",
        "                                batch_size=256,\n",
        "                                epochs=20, \n",
        "                                validation_split = 0.05,\n",
        "                                verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JRBnrGyYr5A2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "epoch = len(train_relu_adam_001.history[\"loss\"])\n",
        "plt.plot(np.arange(1, epoch+1), train_relu_adam_001.history['val_acc'], label='DNN', lw=3)\n",
        "plt.plot(np.arange(1, epoch+1), train_basic_cnn.history['val_acc'], label='Basic CNN', lw=3)\n",
        "plt.plot(np.arange(1, epoch+1), train_model_cnn.history['val_acc'], label='Advanced CNN', lw=3)\n",
        "plt.ylabel('Accuracy', family='serif', size=14)\n",
        "plt.xlabel('Epoch #', family='serif', size=14)\n",
        "plt.xticks(np.arange(1, epoch+1))\n",
        "plt.xlim([1, epoch])\n",
        "plt.legend(prop={'size':14, 'family':'serif'})\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A0H1xKr2XXzr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_result=model_cnn.evaluate(np.expand_dims(test_images, -1)  , test_labels)\n",
        "print(f'Testing Accuracy of the Advanced CNN: {test_result[1]}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gYekeHaJssrw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (Lab 2-k) MobileNet"
      ]
    },
    {
      "metadata": {
        "id": "8vEm6gRCx2qk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.layers import Lambda, Input\n",
        "from tensorflow.keras.models import Model\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nwGgW61oSOC1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "# The minimun size accepted by mobilenet is 32, and our images now is 28x28.\n",
        "# Images would be resizd to 2x.\n",
        "\n",
        "height,width = 56, 56\n",
        "\n",
        "input_image = Input(shape=(height,width))\n",
        "\n",
        "# Mobilenet trained on 3 channel images(RGB). Here, we expand dimension to 3 channel, \n",
        "# and use the value for all 3 channel\n",
        "\n",
        "input_image_ = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,-1),3,3))(input_image)\n",
        "\n",
        "model_mobilenet_base = MobileNet(input_tensor=input_image_,\n",
        "                                 weights='imagenet',\n",
        "                                 include_top=False, \n",
        "                                 pooling='avg')\n",
        "\n",
        "x = Dropout(0.5)(model_mobilenet_base.output)\n",
        "\n",
        "predict = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model_mobilenet = Model(inputs=input_image, outputs=predict)\n",
        "\n",
        "opfunc = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
        "\n",
        "model_mobilenet.compile(optimizer=opfunc, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_mobilenet.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EAWvmDp6lil6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "resized_train_images = np.array([cv2.resize(x, (height, width)).astype(float) for x in train_images])\n",
        "\n",
        "train_mobilenet = model_mobilenet.fit(resized_train_images, \n",
        "                                          train_labels,\n",
        "                                          batch_size=256,\n",
        "                                          epochs=20, \n",
        "                                          validation_split=0.05,                           \n",
        "                                          verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NzAvPNg_x6Bl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "resized_test_images = np.array([cv2.resize(x, (height, width)).astype(float) for x in test_images])\n",
        "test_result = model_mobilenet.evaluate(resized_test_images , test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u3T7Kc35a3Qc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "epoch = len(train_relu_adam_001.history[\"loss\"])\n",
        "\n",
        "plt.plot(np.arange(1, epoch+1), train_relu_adam_001.history['val_acc'], label='DNN', lw=3)\n",
        "plt.plot(np.arange(1, epoch+1), train_basic_cnn.history['val_acc'], label='Basic CNN', lw=3)\n",
        "plt.plot(np.arange(1, epoch+1), train_model_cnn.history['val_acc'], label='Advanced CNN', lw=3)\n",
        "plt.plot(np.arange(1, epoch+1), train_mobilenet.history['val_acc'], label='MobileNet', lw=3)\n",
        "\n",
        "plt.ylabel('Accuracy', family='serif', size=14)\n",
        "plt.xlabel('Epoch #', family='serif', size=14)\n",
        "plt.xticks(np.arange(1, epoch+1))\n",
        "plt.xlim([1, epoch])\n",
        "plt.legend(prop={'size':14, 'family':'serif'})\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BEsI2yU2IJIA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(f'Testing Accuracy of MobileNet: {test_result[1]}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNAK6uZ6GFl5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}