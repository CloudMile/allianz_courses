{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LG76H6DPITSu"
   },
   "source": [
    "# Load Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DfVao3ijITSw"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install seaborn --upgrade\n",
    "\n",
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf\n",
    "import seaborn as sns, keras\n",
    "sns.set(style='white')\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, scale\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Input, Dropout, Embedding, Flatten, Input\n",
    "from keras.layers import dot, add, Lambda, Concatenate, multiply, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, Adagrad\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhBk7C9bITS2"
   },
   "source": [
    "# Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OA-rEJcbITS4"
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('https://storage.googleapis.com/allianz-course/data/ratings.csv')\n",
    "movies = pd.read_csv('https://storage.googleapis.com/allianz-course/data/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNreyKdBITS5"
   },
   "outputs": [],
   "source": [
    "print(movies.shape)\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jTXwbgvZITS9"
   },
   "outputs": [],
   "source": [
    "print(ratings.shape)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wjlestBITTB"
   },
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqydSHqBITTB"
   },
   "outputs": [],
   "source": [
    "# Fit user id and movie id\n",
    "uid_enc, mid_enc = LabelEncoder(), LabelEncoder()\n",
    "uid_enc.fit(ratings.userId)\n",
    "mid_enc.fit(movies.movieId)\n",
    "\n",
    "# Encode user id and movie id to indexed real value\n",
    "ratings[\"userId\"] = uid_enc.transform(ratings.userId)\n",
    "ratings[\"movieId\"] = mid_enc.transform(ratings.movieId)\n",
    "movies[\"movieId\"] = mid_enc.transform(movies.movieId)\n",
    "\n",
    "# Dictionary of movie id and title\n",
    "mid_map = pd.Series(dict(zip(movies.movieId, movies.title)))\n",
    "\n",
    "# Number of users, number of movies\n",
    "n_users, n_movies = len(uid_enc.classes_), len(mid_enc.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAwbYyNcITTE"
   },
   "source": [
    "# Split Train, Test Data\n",
    "* 以4分為閥值, 4分以上為positive, 未滿4分為negative\n",
    "* 每個user分positive, negative兩部分, 各取30%到valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLB40Z00ITTF"
   },
   "outputs": [],
   "source": [
    "def split_ratings(data, pos_thres=4, test_ratio=0.3):\n",
    "    \"\"\"依照test_ratio切割movielens train test資料\"\"\"\n",
    "    tr, te = [], []\n",
    "    for u, df in data.groupby(\"userId\"):\n",
    "        if len(df) < 5: continue\n",
    "\n",
    "        pos, neg = df.query(\"rating >= {}\".format(pos_thres)), df.query(\"rating < {}\".format(pos_thres))\n",
    "        # Split positive part\n",
    "        pos_len = int(len(pos) * (1 - test_ratio))\n",
    "        tr_pos = pos[:pos_len]\n",
    "        te_pos = pos[pos_len:]\n",
    "        # Split negative part\n",
    "        neg_len = int(len(neg) * (1 - test_ratio))\n",
    "        tr_neg = neg[:neg_len]\n",
    "        te_neg = neg[neg_len:]\n",
    "\n",
    "        tr.append(tr_pos.append(tr_neg))\n",
    "        te.append(te_pos.append(te_neg))\n",
    "    return pd.concat(tr, ignore_index=True), pd.concat(te, ignore_index=True)\n",
    "\n",
    "tr, te = split_ratings(ratings, 4, .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nLh9NlnfITTH"
   },
   "source": [
    "# Make Rating Matrix (Interaction Between Users and Movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "plMmmUmDITTI"
   },
   "outputs": [],
   "source": [
    "tr_rating_mat = np.zeros((n_users, n_movies))\n",
    "# Valid data rating matrix\n",
    "te_rating_mat = np.zeros((n_users, n_movies))\n",
    "\n",
    "# Train rating matrix\n",
    "for idx, r in tr.iterrows():\n",
    "    tr_rating_mat[int(r.userId), int(r.movieId)] = r.rating\n",
    "# Valid rating matrix    \n",
    "for idx, r in te.iterrows():\n",
    "    te_rating_mat[int(r.userId), int(r.movieId)] = r.rating\n",
    "    \n",
    "print('Shape of train interaction matrix: ', tr_rating_mat.shape)\n",
    "print(tr_rating_mat, '\\n')\n",
    "print('Shape of test interaction matrix: ', te_rating_mat.shape)\n",
    "print(te_rating_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3KR-fNOITTL"
   },
   "source": [
    "# Encode Movies Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZpLaVwTsITTM"
   },
   "outputs": [],
   "source": [
    "def do_movies(movies):\n",
    "    movies = movies.reset_index(drop=True)\n",
    "    movies[\"genres\"] = movies.genres.str.split(\"\\|\")\n",
    "    genres_cnt = Counter()\n",
    "    movies.genres.map(genres_cnt.update)\n",
    "    genres_map = LabelEncoder()\n",
    "    genres_map.fit( np.array(genres_cnt.most_common())[:, 0] )\n",
    "    movies[\"genres\"] = movies.genres.map(lambda lst: genres_map.transform(lst))\n",
    "    \n",
    "    movies[\"avg_rating\"] = ratings.groupby(\"movieId\").rating.mean()\n",
    "    movies[\"avg_rating\"] = scale(movies.avg_rating.fillna(movies.avg_rating.mean()))\n",
    "    movies[\"freq_rating\"] = ratings.groupby(\"movieId\").size()\n",
    "    movies[\"freq_rating\"] = scale(movies.avg_rating.fillna(movies.freq_rating.median()))\n",
    "    movies[\"year\"] = movies.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n",
    "    movies[\"year\"] = scale(movies.year.fillna(movies.year.mean()))\n",
    "\n",
    "    return movies, genres_map\n",
    "\n",
    "movies_encoded, genres_map = do_movies(movies)\n",
    "n_genres = len(genres_map.classes_)\n",
    "movies_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGQSbZCfITTP"
   },
   "source": [
    "# Encode Users Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8Y5uYFvITTQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# user_encoded \n",
    "user_encoded = ratings.groupby('userId').rating.agg(['size', 'mean'])\n",
    "user_encoded.columns = ['user_rating_freq', 'user_rating_mean']\n",
    "user_encoded['user_rating_freq'] = scale(user_encoded.user_rating_freq)\n",
    "user_encoded['user_rating_mean'] = scale(user_encoded.user_rating_mean)\n",
    "user_encoded = user_encoded.reset_index()\n",
    "user_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLGT50YNITTV"
   },
   "source": [
    "# 以leave one out方式產生 train data, test data\n",
    "1. 每一筆資料有兩部分: [user query] + [item id]\n",
    "2. 每一筆user query 包含所有user movie history, 除了當前的rating movie (candidate movie)\n",
    "3. test data的user query來自於train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZ5GYjFtITTV"
   },
   "outputs": [],
   "source": [
    "def loo_preprocess(data, movies_encoded, train_hist=None, is_train=True):\n",
    "    \"\"\"以leave one out方式產生 train data, test data\"\"\"\n",
    "    queue = []\n",
    "    data = data.merge(movies_encoded, how=\"left\", on=\"movieId\")\n",
    "    data = data.merge(user_encoded, how=\"left\", on=\"userId\")\n",
    "    columns = [\"user_id\", \"query_movie_ids\", \"query_movie_ids_len\", \"user_rating_freq\", \"user_rating_mean\",\n",
    "               \"genres\", \"genres_len\", \"avg_rating\", \"freq_rating\", \"year\", \"candidate_movie_id\",\n",
    "               \"rating\"]\n",
    "    \n",
    "    for u, df in data.groupby(\"userId\"):\n",
    "        df = df.sort_values(\"rating\", ascending=False)\n",
    "        # 抓出user給予正向評價的電影 (>= 4)\n",
    "        if is_train:\n",
    "            fav_movies = set(df.query(\"rating >= 4\").movieId)\n",
    "        else:\n",
    "            fav_movies = set(train_hist.query(f\"userId == {u} and rating >= 4\").movieId)\n",
    "        for i, (_, r) in enumerate(df.iterrows()):\n",
    "            queries = list(fav_movies - set([int(r.movieId)]))\n",
    "            # 對於multivalent的欄位, 需要增加一個column去描述該欄位的長度\n",
    "            queue.append([int(r.userId),\n",
    "                          queries,\n",
    "                          len(queries),\n",
    "                          r.user_rating_freq,\n",
    "                          r.user_rating_mean,\n",
    "                          r.genres,\n",
    "                          len(r.genres),\n",
    "                          r.avg_rating,\n",
    "                          r.freq_rating,\n",
    "                          r.year, \n",
    "                          int(r.movieId), \n",
    "                          r.rating])\n",
    "    return pd.DataFrame(queue, columns=columns)\n",
    "\n",
    "trProcessed = loo_preprocess(tr, movies_encoded)\n",
    "teProcessed = loo_preprocess(te, movies_encoded, tr, is_train=False)\n",
    "trProcessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WnF9eJ1pITTa"
   },
   "outputs": [],
   "source": [
    "teProcessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDq73FZKITTd"
   },
   "source": [
    "## Data Function\n",
    "1. 由於 Keras(Tensorflow backend) 不支援變動長度的columns, 需透過padding zero(補零)帶入\n",
    "2. 每個變動長度的column, 需要再給lens描述每一筆資料的長度, ex: query_movie_ids, query_movie_ids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLsct90sITTe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feats = [\"query_movie_ids\", \"query_movie_ids_len\", \"user_rating_freq\", \"user_rating_mean\",\n",
    "         \"genres\", \"genres_len\", \"avg_rating\", \"freq_rating\", \"year\", \"candidate_movie_id\", 'global']\n",
    "multi_cols = [\"query_movie_ids\", 'genres']\n",
    "label = 'rating'\n",
    "\n",
    "# Generator function\n",
    "def dataFn(data, n_batch=128, shuffle=False):\n",
    "    pad = pad_sequences\n",
    "    def fn():\n",
    "        while True:\n",
    "            dataInner = data.copy()\n",
    "            indices = get_minibatches_idx(len(dataInner), n_batch, shuffle=shuffle)\n",
    "            for ind in indices:\n",
    "                ret = do_multi(dataInner.iloc[ind], multi_cols)\n",
    "                ret['global'] = 0\n",
    "                yield [np.stack(ret[col].values) if col in multi_cols else ret[col][:, None]\n",
    "                       for col in feats], ret.rating.values[:, None]\n",
    "    return fn\n",
    "\n",
    "def get_minibatches_idx(n, batch_size, shuffle=False):\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // batch_size):\n",
    "        minibatches.append(idx_list[minibatch_start : minibatch_start + batch_size])\n",
    "        minibatch_start += batch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "    return minibatches\n",
    "\n",
    "def do_multi(df, multi_cols):\n",
    "    \"\"\"Padding the multivalent feature\"\"\"\n",
    "    pad = pad_sequences\n",
    "    df = df.copy()\n",
    "    for colname in multi_cols:\n",
    "        lens = df[colname].map(len)\n",
    "        df[colname] = list(pad(df[colname], padding=\"post\", maxlen=lens.max()))\n",
    "    return df\n",
    "\n",
    "for data, label in dataFn(trProcessed, n_batch=5, shuffle=True)():\n",
    "    break\n",
    "\n",
    "for name, col in zip(feats, data):\n",
    "    print(f'{name}\\n{col}\\n')\n",
    "print(f'label\\n{label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lp3sSdo8ITTg"
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame()\n",
    "for col, val in zip(feats, data):\n",
    "    if col in multi_cols:\n",
    "        tmp[col] = list(val)\n",
    "    else:\n",
    "        tmp[col] = val.ravel()\n",
    "\n",
    "tmp['rating'] = label.ravel()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pQ-Jfr1ITTk"
   },
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZ6TMIgFITTk"
   },
   "source": [
    "# Model of Matrix Factorization with DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbYJfqpYITTl"
   },
   "source": [
    "## Build Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T0PQH_HRITTm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(n_users, n_movies, emb_size, reg):\n",
    "    # Input tesors\n",
    "    inp_query = Input([None], dtype='int32', name='inp_query')\n",
    "    inp_query_len = Input([1], dtype='int32', name='inp_query_len')\n",
    "    inp_u_freq = Input([1], dtype='float32', name='inp_u_freq')\n",
    "    inp_u_mean = Input([1], dtype='float32', name='inp_u_mean')\n",
    "    inp_genres = Input([None], dtype='int32', name='inp_genres')\n",
    "    inp_genres_len = Input([1], dtype='int32', name='inp_genres_len')\n",
    "    inp_avg_rating = Input([1], dtype='float32', name='inp_avg_rating')\n",
    "    inp_freq_rating = Input([1], dtype='float32', name='inp_freq_rating')\n",
    "    inp_year = Input([1], dtype='float32', name='inp_year')\n",
    "    inp_movie = Input([1], dtype='int32', name='inp_movie')\n",
    "    # Hack: only input integer => \"0\"\n",
    "    inp_global = Input([1], dtype='int32', name='inp_global')\n",
    "    \n",
    "    # User, movie, genres embedding\n",
    "    emb_query = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform', \n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_query)\n",
    "    emb_genres = Embedding(n_genres, 8, embeddings_initializer='glorot_uniform',\n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_genres)\n",
    "    emb_movie = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform',\n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_movie)\n",
    "    \n",
    "    # User side\n",
    "    def sqrtn(x):\n",
    "        qry, lens = x\n",
    "        lens = tf.reshape(lens, [-1])\n",
    "        weights = tf.nn.l2_normalize(tf.sequence_mask(lens, dtype=tf.float32), 1)\n",
    "        weights = tf.expand_dims(weights, -1)\n",
    "        return tf.reduce_sum(qry * weights, 1)\n",
    "    emb_query = Lambda(sqrtn, name='emb_query')([emb_query, inp_query_len])\n",
    "    emb_query = Concatenate(1)([emb_query, inp_u_freq, inp_u_mean])\n",
    "    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_query)\n",
    "    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='user_impression')(emb_query)\n",
    "    \n",
    "    # Movie side\n",
    "    emb_genres = Lambda(sqrtn, name='emb_genres')([emb_genres, inp_genres_len])\n",
    "    emb_movie = Flatten(name='emb_movie')(emb_movie)\n",
    "    emb_movie = Concatenate(1)([emb_movie, emb_genres, inp_avg_rating, inp_freq_rating, inp_year])\n",
    "    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_movie)\n",
    "    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='movie_impression')(emb_movie)\n",
    "    \n",
    "    # Bias terms\n",
    "    # Projection of emb_query to get bias\n",
    "    b_user = Dense(1, \n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   kernel_regularizer=regularizers.l2(reg),\n",
    "                   activation='linear', \n",
    "                   use_bias=False,\n",
    "                   name='b_user')(emb_query)\n",
    "    # Projection of emb_movie to get bias\n",
    "    b_movie = Dense(1, \n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   kernel_regularizer=regularizers.l2(reg),\n",
    "                   activation='linear', \n",
    "                   use_bias=False,\n",
    "                   name='b_movie')(emb_movie)\n",
    "    b_global = Flatten(name='b_global')(Embedding(1, 1, embeddings_initializer='glorot_uniform')(inp_global))\n",
    "    \n",
    "    # Implements the formulation\n",
    "    nets = dot([emb_query, emb_movie], axes=1)\n",
    "    nets = add([nets, b_user, b_movie, b_global])\n",
    "    \n",
    "    model = Model([inp_query, \n",
    "                   inp_query_len, \n",
    "                   inp_u_freq,\n",
    "                   inp_u_mean,\n",
    "                   inp_genres, \n",
    "                   inp_genres_len, \n",
    "                   inp_avg_rating,\n",
    "                   inp_freq_rating,\n",
    "                   inp_year,\n",
    "                   inp_movie, \n",
    "                   inp_global], nets)\n",
    "    model.summary()\n",
    "    return model, Model([inp_movie, \n",
    "                         inp_genres, \n",
    "                         inp_genres_len, \n",
    "                         inp_avg_rating,\n",
    "                         inp_freq_rating,\n",
    "                         inp_year], emb_movie)\n",
    "\n",
    "emb_size = 8\n",
    "reg = 0.0005\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "K.clear_session()\n",
    "model_mf_dnn, model_emb_movie = get_model(n_users, n_movies, emb_size, reg)\n",
    "model_mf_dnn.compile(optimizer=SGD(lr=0.05), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kuMa62RwITTp"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rH_vMs3ITTq"
   },
   "source": [
    "## Use Callback Function\n",
    "* keras.callbacks.ModelCheckpoint: 只存檔最好的結果, 是另一種防止overfitting的方式\n",
    "    * save_best_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dA9S8yPnITTr",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_dir = \"./model_mf_dnn\"\n",
    "\n",
    "tr_len = len(trProcessed)\n",
    "te_len = len(teProcessed)\n",
    "hist = model_mf_dnn.fit_generator(\n",
    "    generator=dataFn(trProcessed, n_batch=batch_size, shuffle=True)(),\n",
    "    steps_per_epoch=tr_len // batch_size + (1 if tr_len % batch_size else 0),\n",
    "    validation_data=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n",
    "    validation_steps=te_len // batch_size + (1 if te_len % batch_size else 0),\n",
    "    # batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[ModelCheckpoint(filepath=model_dir, \n",
    "                               save_weights_only=True, \n",
    "                               save_best_only=True)]\n",
    ")\n",
    "\n",
    "# After training, load the best weights back\n",
    "model_mf_dnn.load_weights(model_dir)\n",
    "\n",
    "sns.lineplot(np.arange(len(hist.history['loss'])), hist.history['loss'], label='train')\n",
    "sns.lineplot(np.arange(len(hist.history['val_loss'])), hist.history['val_loss'], label='test')\n",
    "plt.title('loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3icFptzITTs"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLtz1c9gITTs"
   },
   "outputs": [],
   "source": [
    "te_len = len(teProcessed)\n",
    "pred = model_mf_dnn.predict_generator(\n",
    "    generator=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n",
    "    steps=te_len // batch_size + (1 if te_len % batch_size else 0)\n",
    ").ravel()\n",
    "print('Shape of test data: ', pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGcZRFeUITTv"
   },
   "source": [
    "# Metrics\n",
    "* 定義4分以上為正向評價, 4分以下為負向評價"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IT13YnBMITTw"
   },
   "source": [
    "## RMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cOGf19vITTx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "te_len = len(teProcessed)\n",
    "valis_steps = te_len // batch_size + (1 if te_len % batch_size else 0)\n",
    "\n",
    "te_y = []\n",
    "for i, (feat, label) in enumerate(dataFn(teProcessed, \n",
    "                                         n_batch=batch_size, \n",
    "                                         shuffle=False)(), 1):\n",
    "    if i > valis_steps: break\n",
    "    te_y += label.ravel().tolist()\n",
    "    \n",
    "te_y = np.array(te_y)\n",
    "print(\"RMSE: \", np.sqrt(np.mean((pred - te_y)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eomC8oPCITTz"
   },
   "source": [
    "## AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2-sErYaITT0"
   },
   "outputs": [],
   "source": [
    "def draw_roc_curve(y, pred_proba):\n",
    "    fpr, tpr, _ = roc_curve(y, pred_proba, pos_label=1)\n",
    "    auc_scr = auc(fpr, tpr)\n",
    "    print(\"auc:\", auc_scr)\n",
    "    f, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.plot(fpr, tpr, label='ROC CURVE')\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.set_title('Area Under Curve(ROC) (score: {:.4f})'.format(auc_scr))\n",
    "    ax.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "draw_roc_curve(te.rating >= 4, pred / pred.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjNLQrCYITT3"
   },
   "source": [
    "## Single User Rating Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKEv6ZgHITT5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# user id from 0 ~ 670\n",
    "uid = 22\n",
    "tmp = teProcessed.query(f\"user_id == {uid}\")\n",
    "single_pred = model_mf_dnn.predict_generator(\n",
    "    generator=dataFn(tmp, n_batch=batch_size, shuffle=False)(),\n",
    "    steps=len(tmp) // batch_size + (1 if len(tmp) % batch_size else 0)\n",
    ").ravel()\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"pred distribute\")\n",
    "sns.distplot(single_pred, ax=ax[0])\n",
    "ax[1].set_title(\"real distribute\")\n",
    "sns.distplot(te.query(f\"userId == '{uid}'\").rating, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l5DpJZcaITT8"
   },
   "source": [
    "## Single User Detail Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8bNNaT5ITT9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# user id from 0 ~ 670\n",
    "uid = 22\n",
    "tmp = teProcessed.query(f\"user_id == {uid}\")\n",
    "single_pred = model_mf_dnn.predict_generator(\n",
    "    generator=dataFn(tmp, n_batch=batch_size, shuffle=False)(),\n",
    "    steps=len(tmp) // batch_size + (1 if len(tmp) % batch_size else 0)\n",
    ").ravel()\n",
    "\n",
    "recommDf = pd.DataFrame(data={\n",
    "              \"userId\": uid,\n",
    "              \"movieId\": tmp.candidate_movie_id,\n",
    "              \"title\": mid_map[tmp.candidate_movie_id].values,\n",
    "              \"rating\": tmp.rating.values,\n",
    "              \"predRating\": single_pred},\n",
    "             columns=(\"userId\", \"movieId\", \"title\", \"rating\", \"predRating\"))\n",
    "# ascending 可以調整True or False觀察結果\n",
    "recommDf.sort_values(\"rating\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpiqgPKGITT_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommDf.sort_values(\"predRating\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQgIzaZ4ITUC"
   },
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "# 利用 Movie Embedding 找出相似電影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Snfb5OUITUC"
   },
   "outputs": [],
   "source": [
    "movies[movies.title.str.contains(\"Toy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xza_nN49ITUF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_mf_dnn.load_weights(model_dir)\n",
    "\n",
    "# Movie data function generator\n",
    "movies_cols = ['movieId', 'genres', 'genres_len', 'avg_rating', 'freq_rating', 'year']\n",
    "def movie_data_fn(data, batch_size=128):\n",
    "    def _fn():\n",
    "        data_inner = data.copy()\n",
    "        while True:\n",
    "            indices = get_minibatches_idx(len(data_inner), batch_size, shuffle=False)\n",
    "            for ind in indices:\n",
    "                ret = do_multi(data_inner.iloc[ind], ['genres'])\n",
    "                ret['global'] = 0\n",
    "                yield [np.stack(ret[col].values) if col in ['genres'] else ret[col][:, None]\n",
    "                       for col in movies_cols]\n",
    "    return _fn\n",
    "\n",
    "def most_like(model, seed_movie, k=10):\n",
    "    \"\"\"給定某一部電影, 使用model裡movies embedding找尋cosine相似度高的其他電影!\"\"\"\n",
    "    tmp = movies_encoded.copy()\n",
    "    tmp['genres_len'] = tmp.genres.map(len)\n",
    "    movie_emb = model.predict_generator(\n",
    "        generator=movie_data_fn(tmp, batch_size)(),\n",
    "        steps=len(tmp) // batch_size + (1 if len(tmp) % batch_size else 0)\n",
    "    )\n",
    "    # print(cosine_similarity(movie_emb[seed_movie][np.newaxis, :], movie_emb))\n",
    "    most_like = cosine_similarity(movie_emb[seed_movie][np.newaxis, :], movie_emb).ravel().argsort()[::-1][:k]\n",
    "    return movies.iloc[most_like]\n",
    "\n",
    "# mse訓練出來的model\n",
    "most_like(model_emb_movie, 7575, k=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iz7Y1uDfITUI"
   },
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "# (LAB) 將Model從Regression改為Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLjmDNf6ITUJ"
   },
   "source": [
    "## Modify Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JSFhNpGITUJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generator function\n",
    "def dataFn(data, n_batch=128, shuffle=False):\n",
    "    pad = pad_sequences\n",
    "    def fn():\n",
    "        while True:\n",
    "            dataInner = data.copy()\n",
    "            indices = get_minibatches_idx(len(dataInner), n_batch, shuffle=shuffle)\n",
    "            for ind in indices:\n",
    "                ret = do_multi(dataInner.iloc[ind], multi_cols)\n",
    "                ret['global'] = 0\n",
    "                yield [np.stack(ret[col].values) if col in multi_cols else ret[col][:, None]\n",
    "                       for col in feats], (ret.rating >= 4).astype(int)[:, None]\n",
    "    return fn\n",
    "\n",
    "for data, label in dataFn(trProcessed, n_batch=5, shuffle=False)():\n",
    "    break\n",
    "\n",
    "print(f'label\\n{label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9nZQGkKITUM"
   },
   "outputs": [],
   "source": [
    "def get_model(n_users, n_movies, emb_size, reg):\n",
    "    # Input tesors\n",
    "    inp_query = Input([None], dtype='int32', name='inp_query')\n",
    "    inp_query_len = Input([1], dtype='int32', name='inp_query_len')\n",
    "    inp_u_freq = Input([1], dtype='float32', name='inp_u_freq')\n",
    "    inp_u_mean = Input([1], dtype='float32', name='inp_u_mean')\n",
    "    inp_genres = Input([None], dtype='int32', name='inp_genres')\n",
    "    inp_genres_len = Input([1], dtype='int32', name='inp_genres_len')\n",
    "    inp_avg_rating = Input([1], dtype='float32', name='inp_avg_rating')\n",
    "    inp_freq_rating = Input([1], dtype='float32', name='inp_freq_rating')\n",
    "    inp_year = Input([1], dtype='float32', name='inp_year')\n",
    "    inp_movie = Input([1], dtype='int32', name='inp_movie')\n",
    "    # Hack: only input integer => \"0\"\n",
    "    inp_global = Input([1], dtype='int32', name='inp_global')\n",
    "    \n",
    "    # User, movie, genres embedding\n",
    "    emb_query = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform', \n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_query)\n",
    "    emb_genres = Embedding(n_genres, 8, embeddings_initializer='glorot_uniform',\n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_genres)\n",
    "    emb_movie = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform',\n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_movie)\n",
    "    \n",
    "    # User side\n",
    "    def sqrtn(x):\n",
    "        qry, lens = x\n",
    "        lens = tf.reshape(lens, [-1])\n",
    "        weights = tf.nn.l2_normalize(tf.sequence_mask(lens, dtype=tf.float32), 1)\n",
    "        weights = tf.expand_dims(weights, -1)\n",
    "        return tf.reduce_sum(qry * weights, 1)\n",
    "    emb_query = Lambda(sqrtn, name='emb_query')([emb_query, inp_query_len])\n",
    "    emb_query = Concatenate(1)([emb_query, inp_u_freq, inp_u_mean])\n",
    "    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_query)\n",
    "    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='user_impression')(emb_query)\n",
    "    \n",
    "    # Movie side\n",
    "    emb_genres = Lambda(sqrtn, name='emb_genres')([emb_genres, inp_genres_len])\n",
    "    emb_movie = Flatten(name='emb_movie')(emb_movie)\n",
    "    emb_movie = Concatenate(1)([emb_movie, emb_genres, inp_avg_rating, inp_freq_rating, inp_year])\n",
    "    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_movie)\n",
    "    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='movie_impression')(emb_movie)\n",
    "    \n",
    "    # Bias terms\n",
    "    # Projection of emb_query to get bias\n",
    "    b_user = Dense(1, \n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   kernel_regularizer=regularizers.l2(reg),\n",
    "                   activation='linear', \n",
    "                   use_bias=False,\n",
    "                   name='b_user')(emb_query)\n",
    "    # Projection of emb_movie to get bias\n",
    "    b_movie = Dense(1, \n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   kernel_regularizer=regularizers.l2(reg),\n",
    "                   activation='linear', \n",
    "                   use_bias=False,\n",
    "                   name='b_movie')(emb_movie)\n",
    "    b_global = Flatten(name='b_global')(Embedding(1, 1, embeddings_initializer='glorot_uniform')(inp_global))\n",
    "    \n",
    "    # Implements the formulation\n",
    "    nets = dot([emb_query, emb_movie], axes=1)\n",
    "    nets = add([nets, b_user, b_movie, b_global])\n",
    "    \n",
    "    ###### START CODE HERE ######\n",
    "    # Modify the model prediction to 0 ~ 1, hint: add an activation function\n",
    "    # ...\n",
    "    ###### END CODE HERE ######\n",
    "    \n",
    "    model = Model([inp_query, \n",
    "                   inp_query_len, \n",
    "                   inp_u_freq,\n",
    "                   inp_u_mean,\n",
    "                   inp_genres, \n",
    "                   inp_genres_len, \n",
    "                   inp_avg_rating,\n",
    "                   inp_freq_rating,\n",
    "                   inp_year,\n",
    "                   inp_movie, \n",
    "                   inp_global], nets)\n",
    "    model.summary()\n",
    "    return model, Model([inp_movie, \n",
    "                         inp_genres, \n",
    "                         inp_genres_len, \n",
    "                         inp_avg_rating,\n",
    "                         inp_freq_rating,\n",
    "                         inp_year], emb_movie)\n",
    "\n",
    "###### START CODE HERE ######\n",
    "# Modify the hyper parameters to get even better result\n",
    "emb_size = # 8, 10, 16 ...\n",
    "reg = # 0.01, 0.005, 0.0005 ...\n",
    "batch_size = 128\n",
    "epochs =  # 10, 20 , 30 ...\n",
    "lr = # 0.1, 0.05, 0.001\n",
    "###### END CODE HERE ######\n",
    "\n",
    "model_dir = \"./model_mf_dnn\"\n",
    "K.clear_session()\n",
    "model_mf_dnn, model_emb_movie = get_model(n_users, n_movies, emb_size, reg)\n",
    "\n",
    "###### START CODE HERE ######\n",
    "# Find best optimizer, e.g: Adam, SGD, Adagrad, find proper loss function\n",
    "# model_mf_dnn.compile(...)\n",
    "###### END CODE HERE ######\n",
    "\n",
    "tr_len = len(trProcessed)\n",
    "te_len = len(teProcessed)\n",
    "hist = model_mf_dnn.fit_generator(\n",
    "    generator=dataFn(trProcessed, n_batch=batch_size, shuffle=True)(),\n",
    "    steps_per_epoch=tr_len // batch_size + (1 if tr_len % batch_size else 0),\n",
    "    validation_data=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n",
    "    validation_steps=te_len // batch_size + (1 if te_len % batch_size else 0),\n",
    "    # batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[ModelCheckpoint(filepath=model_dir, \n",
    "                               save_weights_only=True, \n",
    "                               save_best_only=True)]\n",
    ")\n",
    "\n",
    "# After training, load the best weights back\n",
    "model_mf_dnn.load_weights(model_dir)\n",
    "\n",
    "sns.lineplot(np.arange(len(hist.history['loss'])), hist.history['loss'], label='train')\n",
    "sns.lineplot(np.arange(len(hist.history['val_loss'])), hist.history['val_loss'], label='test')\n",
    "plt.title('loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Prediction\n",
    "pred = model_mf_dnn.predict_generator(\n",
    "    generator=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n",
    "    steps=te_len // batch_size + (1 if te_len % batch_size else 0)\n",
    ").ravel()\n",
    "\n",
    "\n",
    "te_labels = (teProcessed.rating >= 4).astype(int)\n",
    "\n",
    "# AUC\n",
    "print('Shape of test data: ', pred.shape)\n",
    "draw_roc_curve(te_labels, pred)\n",
    "\n",
    "# Confusion matrix, classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print('accuracy_score: ', accuracy_score(te_labels, pred >= 0.5))\n",
    "print(confusion_matrix(te_labels, pred >= 0.5))\n",
    "print()\n",
    "print(classification_report(te_labels, pred >= 0.5))\n",
    "\n",
    "most_like(model_emb_movie, 8787, k=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juY1cFwvITUO"
   },
   "source": [
    "## 利用Movie Embedding, 以Cosine Similarity找出前10名相似電影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3UXF7-nITUO"
   },
   "outputs": [],
   "source": [
    "movies[movies.title.str.contains(\"Inception\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPA3TWFGITUQ"
   },
   "outputs": [],
   "source": [
    "# Call most_like function 找出前10名相似電影\n",
    "# most_like(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uzQ0kyeITUS"
   },
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUrdU2WyITUT"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0sPoeChITUW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(n_users, n_movies, emb_size, reg):\n",
    "    # Input tesors\n",
    "    inp_query = Input([None], dtype='int32', name='inp_query')\n",
    "    inp_query_len = Input([1], dtype='int32', name='inp_query_len')\n",
    "    inp_u_freq = Input([1], dtype='float32', name='inp_u_freq')\n",
    "    inp_u_mean = Input([1], dtype='float32', name='inp_u_mean')\n",
    "    inp_genres = Input([None], dtype='int32', name='inp_genres')\n",
    "    inp_genres_len = Input([1], dtype='int32', name='inp_genres_len')\n",
    "    inp_avg_rating = Input([1], dtype='float32', name='inp_avg_rating')\n",
    "    inp_freq_rating = Input([1], dtype='float32', name='inp_freq_rating')\n",
    "    inp_year = Input([1], dtype='float32', name='inp_year')\n",
    "    inp_movie = Input([1], dtype='int32', name='inp_movie')\n",
    "    # Hack: only input integer => \"0\"\n",
    "    inp_global = Input([1], dtype='int32', name='inp_global')\n",
    "    \n",
    "    # User, movie, genres embedding\n",
    "    emb_query = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform', \n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_query)\n",
    "    emb_genres = Embedding(n_genres, 8, embeddings_initializer='glorot_uniform',\n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_genres)\n",
    "    emb_movie = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform',\n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_movie)\n",
    "    \n",
    "    # User side\n",
    "    def sqrtn(x):\n",
    "        qry, lens = x\n",
    "        lens = tf.reshape(lens, [-1])\n",
    "        weights = tf.nn.l2_normalize(tf.sequence_mask(lens, dtype=tf.float32), 1)\n",
    "        weights = tf.expand_dims(weights, -1)\n",
    "        return tf.reduce_sum(qry * weights, 1)\n",
    "    emb_query = Lambda(sqrtn, name='emb_query')([emb_query, inp_query_len])\n",
    "    emb_query = Concatenate(1)([emb_query, inp_u_freq, inp_u_mean])\n",
    "    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_query)\n",
    "    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='user_impression')(emb_query)\n",
    "    \n",
    "    # Movie side\n",
    "    emb_genres = Lambda(sqrtn, name='emb_genres')([emb_genres, inp_genres_len])\n",
    "    emb_movie = Flatten(name='emb_movie')(emb_movie)\n",
    "    emb_movie = Concatenate(1)([emb_movie, emb_genres, inp_avg_rating, inp_freq_rating, inp_year])\n",
    "    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_movie)\n",
    "    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='movie_impression')(emb_movie)\n",
    "    \n",
    "    # Bias terms\n",
    "    # Projection of emb_query to get bias\n",
    "    b_user = Dense(1, \n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   kernel_regularizer=regularizers.l2(reg),\n",
    "                   activation='linear', \n",
    "                   use_bias=False,\n",
    "                   name='b_user')(emb_query)\n",
    "    # Projection of emb_movie to get bias\n",
    "    b_movie = Dense(1, \n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   kernel_regularizer=regularizers.l2(reg),\n",
    "                   activation='linear', \n",
    "                   use_bias=False,\n",
    "                   name='b_movie')(emb_movie)\n",
    "    b_global = Flatten(name='b_global')(Embedding(1, 1, embeddings_initializer='glorot_uniform')(inp_global))\n",
    "    \n",
    "    # Implements the formulation\n",
    "    nets = dot([emb_query, emb_movie], axes=1)\n",
    "    nets = add([nets, b_user, b_movie, b_global])\n",
    "    \n",
    "    ###### START CODE HERE ######\n",
    "    # Modify the model prediction to 0 ~ 1, hint: add an activation function\n",
    "    nets = Activation(\"sigmoid\")(nets)\n",
    "    ###### END CODE HERE ######\n",
    "    \n",
    "    model = Model([inp_query, \n",
    "                   inp_query_len, \n",
    "                   inp_u_freq,\n",
    "                   inp_u_mean,\n",
    "                   inp_genres, \n",
    "                   inp_genres_len, \n",
    "                   inp_avg_rating,\n",
    "                   inp_freq_rating,\n",
    "                   inp_year,\n",
    "                   inp_movie, \n",
    "                   inp_global], nets)\n",
    "    model.summary()\n",
    "    return model, Model([inp_movie, \n",
    "                         inp_genres, \n",
    "                         inp_genres_len, \n",
    "                         inp_avg_rating,\n",
    "                         inp_freq_rating,\n",
    "                         inp_year], emb_movie)\n",
    "\n",
    "###### START CODE HERE ######\n",
    "# Modify the hyper parameters to get even better result\n",
    "emb_size = 16\n",
    "reg = 0.0005\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "lr = 0.05\n",
    "###### END CODE HERE ######\n",
    "\n",
    "model_dir = \"./model_mf_dnn\"\n",
    "K.clear_session()\n",
    "model_mf_dnn, model_emb_movie = get_model(n_users, n_movies, emb_size, reg)\n",
    "\n",
    "model_mf_dnn.compile(optimizer=SGD(lr=lr), loss='binary_crossentropy')\n",
    "\n",
    "tr_len = len(trProcessed)\n",
    "te_len = len(teProcessed)\n",
    "hist = model_mf_dnn.fit_generator(\n",
    "    generator=dataFn(trProcessed, n_batch=batch_size, shuffle=True)(),\n",
    "    steps_per_epoch=tr_len // batch_size + (1 if tr_len % batch_size else 0),\n",
    "    validation_data=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n",
    "    validation_steps=te_len // batch_size + (1 if te_len % batch_size else 0),\n",
    "    # batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[ModelCheckpoint(filepath=model_dir, \n",
    "                               save_weights_only=True, \n",
    "                               save_best_only=True)]\n",
    ")\n",
    "\n",
    "# After training, load the best weights back\n",
    "model_mf_dnn.load_weights(model_dir)\n",
    "\n",
    "sns.lineplot(np.arange(len(hist.history['loss'])), hist.history['loss'], label='train')\n",
    "sns.lineplot(np.arange(len(hist.history['val_loss'])), hist.history['val_loss'], label='test')\n",
    "plt.title('loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Prediction\n",
    "pred = model_mf_dnn.predict_generator(\n",
    "    generator=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n",
    "    steps=te_len // batch_size + (1 if te_len % batch_size else 0)\n",
    ").ravel()\n",
    "\n",
    "te_labels = (teProcessed.rating >= 4).astype(int)\n",
    "# AUC\n",
    "print('Shape of test data: ', pred.shape)\n",
    "draw_roc_curve(te_labels, pred)\n",
    "\n",
    "# Confusion matrix, classification report\n",
    "print('accuracy_score: ', accuracy_score(te_labels, pred >= 0.5))\n",
    "print(confusion_matrix(te_labels, pred >= 0.5))\n",
    "print()\n",
    "print(classification_report(te_labels, pred >= 0.5))\n",
    "\n",
    "most_like(model_emb_movie, 0, k=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hWhb52yITUZ"
   },
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "# (LAB) 延續Classification, 以DNN作法取代MF作法\n",
    "\n",
    "* Concatenate [user, movie], 且用 dense layer 增加hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZ87JqpYITUZ"
   },
   "outputs": [],
   "source": [
    "def get_model(n_users, n_movies, emb_size, reg):\n",
    "    # Input tesors\n",
    "    inp_query = Input([None], dtype='int32', name='inp_query')\n",
    "    inp_query_len = Input([1], dtype='int32', name='inp_query_len')\n",
    "    inp_u_freq = Input([1], dtype='float32', name='inp_u_freq')\n",
    "    inp_u_mean = Input([1], dtype='float32', name='inp_u_mean')\n",
    "    inp_genres = Input([None], dtype='int32', name='inp_genres')\n",
    "    inp_genres_len = Input([1], dtype='int32', name='inp_genres_len')\n",
    "    inp_avg_rating = Input([1], dtype='float32', name='inp_avg_rating')\n",
    "    inp_freq_rating = Input([1], dtype='float32', name='inp_freq_rating')\n",
    "    inp_year = Input([1], dtype='float32', name='inp_year')\n",
    "    inp_movie = Input([1], dtype='int32', name='inp_movie')\n",
    "    # Hack: only input integer => \"0\"\n",
    "    inp_global = Input([1], dtype='int32', name='inp_global')\n",
    "    \n",
    "    # User, movie, genres embedding\n",
    "    emb_query = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform', \n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_query)\n",
    "    emb_genres = Embedding(n_genres, 8, embeddings_initializer='glorot_uniform',\n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_genres)\n",
    "    emb_movie = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform',\n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_movie)\n",
    "    \n",
    "    # User side\n",
    "    def sqrtn(x):\n",
    "        qry, lens = x\n",
    "        lens = tf.reshape(lens, [-1])\n",
    "        weights = tf.nn.l2_normalize(tf.sequence_mask(lens, dtype=tf.float32), 1)\n",
    "        weights = tf.expand_dims(weights, -1)\n",
    "        return tf.reduce_sum(qry * weights, 1)\n",
    "    emb_query = Lambda(sqrtn, name='emb_query')([emb_query, inp_query_len])\n",
    "    emb_query = Concatenate(1)([emb_query, inp_u_freq, inp_u_mean])\n",
    "    emb_query = Dense(emb_size, activation='relu')(emb_query)\n",
    "    emb_query = Dense(emb_size, activation='relu', name='user_impression')(emb_query)\n",
    "    \n",
    "    # Movie side\n",
    "    emb_genres = Lambda(sqrtn, name='emb_genres')([emb_genres, inp_genres_len])\n",
    "    emb_movie = Flatten(name='emb_movie')(emb_movie)\n",
    "    emb_movie = Concatenate(1)([emb_movie, emb_genres, inp_avg_rating, inp_freq_rating, inp_year])\n",
    "    emb_movie = Dense(emb_size, activation='relu')(emb_movie)\n",
    "    emb_movie = Dense(emb_size, activation='relu', name='movie_impression')(emb_movie)\n",
    "    \n",
    "    ###### START CODE HERE ######\n",
    "    # Concatenate the user and movie embedding, add 2 hidden layers\n",
    "    # Maybe you need Dropout! BatchNormalization, etc..., try it for the better result!\n",
    "    # nets = Concatenate(1)([emb_query, emb_movie])\n",
    "    # nets = Dense(...)(nets)\n",
    "    # ...\n",
    "    ###### END CODE HERE ######\n",
    "    \n",
    "    model = Model([inp_query, \n",
    "                   inp_query_len, \n",
    "                   inp_u_freq,\n",
    "                   inp_u_mean,\n",
    "                   inp_genres, \n",
    "                   inp_genres_len, \n",
    "                   inp_avg_rating,\n",
    "                   inp_freq_rating,\n",
    "                   inp_year,\n",
    "                   inp_movie, \n",
    "                   inp_global], nets)\n",
    "    model.summary()\n",
    "    return model, Model([inp_movie, \n",
    "                         inp_genres, \n",
    "                         inp_genres_len, \n",
    "                         inp_avg_rating,\n",
    "                         inp_freq_rating,\n",
    "                         inp_year], emb_movie)\n",
    "\n",
    "###### START CODE HERE ######\n",
    "# Modify the hyper parameters to get even better result\n",
    "emb_size = # 8, 10, 16 ...\n",
    "reg = # 0.01, 0.005, 0.0005 ...\n",
    "batch_size = 128\n",
    "epochs =  # 10, 20 , 30 ...\n",
    "lr = # 0.1, 0.05, 0.001\n",
    "###### END CODE HERE ######\n",
    "\n",
    "model_dir = \"./model_mf_dnn\"\n",
    "K.clear_session()\n",
    "model_mf_dnn, model_emb_movie = get_model(n_users, n_movies, emb_size, reg)\n",
    "\n",
    "###### START CODE HERE ######\n",
    "# Find best optimizer, e.g: Adam, SGD, Adagrad, find proper loss function\n",
    "# model_mf.compile(...)\n",
    "###### END CODE HERE ######\n",
    "\n",
    "tr_len = len(trProcessed)\n",
    "te_len = len(teProcessed)\n",
    "hist = model_mf_dnn.fit_generator(\n",
    "    generator=dataFn(trProcessed, n_batch=batch_size, shuffle=True)(),\n",
    "    steps_per_epoch=tr_len // batch_size + (1 if tr_len % batch_size else 0),\n",
    "    validation_data=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n",
    "    validation_steps=te_len // batch_size + (1 if te_len % batch_size else 0),\n",
    "    # batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[ModelCheckpoint(filepath=model_dir, \n",
    "                               save_weights_only=True, \n",
    "                               save_best_only=True)]\n",
    ")\n",
    "\n",
    "# After training, load the best weights back\n",
    "model_mf_dnn.load_weights(model_dir)\n",
    "\n",
    "sns.lineplot(np.arange(len(hist.history['loss'])), hist.history['loss'], label='train')\n",
    "sns.lineplot(np.arange(len(hist.history['val_loss'])), hist.history['val_loss'], label='test')\n",
    "plt.title('loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Prediction\n",
    "te_len = len(teProcessed)\n",
    "pred = model_mf_dnn.predict_generator(\n",
    "    generator=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n",
    "    steps=te_len // batch_size + (1 if te_len % batch_size else 0)\n",
    ").ravel()\n",
    "\n",
    "\n",
    "te_labels = (teProcessed.rating >= 4).astype(int)\n",
    "# AUC\n",
    "print('Shape of test data: ', pred.shape)\n",
    "draw_roc_curve(te_labels, pred)\n",
    "\n",
    "# Confusion matrix, classification report\n",
    "print('accuracy_score: ', accuracy_score(te_labels, pred >= 0.5))\n",
    "print(confusion_matrix(te_labels, pred >= 0.5))\n",
    "print()\n",
    "print(classification_report(te_labels, pred >= 0.5))\n",
    "\n",
    "most_like(model_emb_movie, 8787, k=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrN1s42DITUc"
   },
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HICHEu6jITUc"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0E5CyPWITUd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(n_users, n_movies, emb_size, reg):\n",
    "    # Input tesors\n",
    "    inp_query = Input([None], dtype='int32', name='inp_query')\n",
    "    inp_query_len = Input([1], dtype='int32', name='inp_query_len')\n",
    "    inp_u_freq = Input([1], dtype='float32', name='inp_u_freq')\n",
    "    inp_u_mean = Input([1], dtype='float32', name='inp_u_mean')\n",
    "    inp_genres = Input([None], dtype='int32', name='inp_genres')\n",
    "    inp_genres_len = Input([1], dtype='int32', name='inp_genres_len')\n",
    "    inp_avg_rating = Input([1], dtype='float32', name='inp_avg_rating')\n",
    "    inp_freq_rating = Input([1], dtype='float32', name='inp_freq_rating')\n",
    "    inp_year = Input([1], dtype='float32', name='inp_year')\n",
    "    inp_movie = Input([1], dtype='int32', name='inp_movie')\n",
    "    # Hack: only input integer => \"0\"\n",
    "    inp_global = Input([1], dtype='int32', name='inp_global')\n",
    "    \n",
    "    # User, movie, genres embedding\n",
    "    emb_query = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform', \n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_query)\n",
    "    emb_genres = Embedding(n_genres, 8, embeddings_initializer='glorot_uniform',\n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_genres)\n",
    "    emb_movie = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform',\n",
    "                          embeddings_regularizer=regularizers.l2(reg))(inp_movie)\n",
    "    \n",
    "    # User side\n",
    "    def sqrtn(x):\n",
    "        qry, lens = x\n",
    "        lens = tf.reshape(lens, [-1])\n",
    "        weights = tf.nn.l2_normalize(tf.sequence_mask(lens, dtype=tf.float32), 1)\n",
    "        weights = tf.expand_dims(weights, -1)\n",
    "        return tf.reduce_sum(qry * weights, 1)\n",
    "    emb_query = Lambda(sqrtn, name='emb_query')([emb_query, inp_query_len])\n",
    "    emb_query = Concatenate(1)([emb_query, inp_u_freq, inp_u_mean])\n",
    "    emb_query = Dense(emb_size, activation='relu')(emb_query)\n",
    "    emb_query = Dense(emb_size, activation='relu', name='user_impression')(emb_query)\n",
    "    \n",
    "    # Movie side\n",
    "    emb_genres = Lambda(sqrtn, name='emb_genres')([emb_genres, inp_genres_len])\n",
    "    emb_movie = Flatten(name='emb_movie')(emb_movie)\n",
    "    emb_movie = Concatenate(1)([emb_movie, emb_genres, inp_avg_rating, inp_freq_rating, inp_year])\n",
    "    emb_movie = Dense(emb_size, activation='relu')(emb_movie)\n",
    "    emb_movie = Dense(emb_size, activation='relu', name='movie_impression')(emb_movie)\n",
    "    \n",
    "    ###### START CODE HERE ######\n",
    "    # Concatenate the user and movie embedding, add 2 hidden layers\n",
    "    # Maybe you need Dropout! BatchNormalization, etc..., try it for the better result!\n",
    "    nets = Concatenate(1)([emb_query, emb_movie])\n",
    "    nets = Dense(32, activation='linear')(nets)\n",
    "    nets = Activation('relu')(BatchNormalization()(nets))\n",
    "    nets = Dense(16, activation='linear')(nets)\n",
    "    nets = Activation('relu')(BatchNormalization()(nets))\n",
    "    nets = Dense(1, activation='sigmoid')(nets)\n",
    "    ###### END CODE HERE ######\n",
    "    \n",
    "    model = Model([inp_query, \n",
    "                   inp_query_len, \n",
    "                   inp_u_freq,\n",
    "                   inp_u_mean,\n",
    "                   inp_genres, \n",
    "                   inp_genres_len, \n",
    "                   inp_avg_rating,\n",
    "                   inp_freq_rating,\n",
    "                   inp_year,\n",
    "                   inp_movie, \n",
    "                   inp_global], nets)\n",
    "    model.summary()\n",
    "    return model, Model([inp_movie, \n",
    "                         inp_genres, \n",
    "                         inp_genres_len, \n",
    "                         inp_avg_rating,\n",
    "                         inp_freq_rating,\n",
    "                         inp_year], emb_movie)\n",
    "\n",
    "###### START CODE HERE ######\n",
    "# Modify the hyper parameters to get even better result\n",
    "emb_size = 16\n",
    "reg = 0.0005\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "lr = 0.05\n",
    "###### END CODE HERE ######\n",
    "\n",
    "model_dir = \"./model_mf_dnn\"\n",
    "K.clear_session()\n",
    "model_mf_dnn, model_emb_movie = get_model(n_users, n_movies, emb_size, reg)\n",
    "\n",
    "model_mf_dnn.compile(optimizer=SGD(lr=lr), loss='binary_crossentropy')\n",
    "\n",
    "tr_len = len(trProcessed)\n",
    "te_len = len(teProcessed)\n",
    "hist = model_mf_dnn.fit_generator(\n",
    "    generator=dataFn(trProcessed, n_batch=batch_size, shuffle=True)(),\n",
    "    steps_per_epoch=tr_len // batch_size + (1 if tr_len % batch_size else 0),\n",
    "    validation_data=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n",
    "    validation_steps=te_len // batch_size + (1 if te_len % batch_size else 0),\n",
    "    # batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[ModelCheckpoint(filepath=model_dir, \n",
    "                               save_weights_only=True, \n",
    "                               save_best_only=True)]\n",
    ")\n",
    "\n",
    "# After training, load the best weights back\n",
    "model_mf_dnn.load_weights(model_dir)\n",
    "\n",
    "sns.lineplot(np.arange(len(hist.history['loss'])), hist.history['loss'], label='train')\n",
    "sns.lineplot(np.arange(len(hist.history['val_loss'])), hist.history['val_loss'], label='test')\n",
    "plt.title('loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Prediction\n",
    "te_len = len(teProcessed)\n",
    "pred = model_mf_dnn.predict_generator(\n",
    "    generator=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n",
    "    steps=te_len // batch_size + (1 if te_len % batch_size else 0)\n",
    ").ravel()\n",
    "\n",
    "\n",
    "te_labels = (teProcessed.rating >= 4).astype(int)\n",
    "# AUC\n",
    "print('Shape of test data: ', pred.shape)\n",
    "draw_roc_curve(te_labels, pred)\n",
    "\n",
    "# Confusion matrix, classification report\n",
    "print('accuracy_score: ', accuracy_score(te_labels, pred >= 0.5))\n",
    "print(confusion_matrix(te_labels, pred >= 0.5))\n",
    "print()\n",
    "print(classification_report(te_labels, pred >= 0.5))\n",
    "\n",
    "most_like(model_emb_movie, 0, k=11)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab_reco_model_mf_dnn.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
