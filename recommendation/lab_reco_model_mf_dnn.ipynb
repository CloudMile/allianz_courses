{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab_reco_model_mf_dnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"LG76H6DPITSu","colab_type":"text"},"cell_type":"markdown","source":["# Load Libs"]},{"metadata":{"id":"DfVao3ijITSw","colab_type":"code","colab":{}},"cell_type":"code","source":["%reload_ext autoreload\n","%autoreload 2\n","\n","!pip install seaborn --upgrade\n","\n","import os, sys, numpy as np, pandas as pd, tensorflow as tf\n","import seaborn as sns, keras\n","sns.set(style='white')\n","\n","from collections import Counter, OrderedDict\n","from matplotlib import pyplot as plt\n","\n","from sklearn.preprocessing import LabelEncoder, scale\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from keras import backend as K\n","from keras.models import Model\n","from keras.layers import Dense, Activation, Input, Dropout, Embedding, Flatten, Input\n","from keras.layers import dot, add, Lambda, Concatenate, multiply, BatchNormalization\n","from keras.optimizers import Adam, SGD, Adagrad\n","from keras import regularizers\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.sequence import pad_sequences\n","\n","np.set_printoptions(precision=4, suppress=True, linewidth=100)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nhBk7C9bITS2","colab_type":"text"},"cell_type":"markdown","source":["# Data Preview"]},{"metadata":{"id":"OA-rEJcbITS4","colab_type":"code","colab":{}},"cell_type":"code","source":["ratings = pd.read_csv('https://storage.googleapis.com/allianz-course/data/ratings.csv')\n","movies = pd.read_csv('https://storage.googleapis.com/allianz-course/data/movies.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PNreyKdBITS5","colab_type":"code","colab":{}},"cell_type":"code","source":["print(movies.shape)\n","movies.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jTXwbgvZITS9","colab_type":"code","colab":{}},"cell_type":"code","source":["print(ratings.shape)\n","ratings.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9wjlestBITTB","colab_type":"text"},"cell_type":"markdown","source":["# Encode"]},{"metadata":{"id":"tqydSHqBITTB","colab_type":"code","colab":{}},"cell_type":"code","source":["# Fit user id and movie id\n","uid_enc, mid_enc = LabelEncoder(), LabelEncoder()\n","uid_enc.fit(ratings.userId)\n","mid_enc.fit(movies.movieId)\n","\n","# Encode user id and movie id to indexed real value\n","ratings[\"userId\"] = uid_enc.transform(ratings.userId)\n","ratings[\"movieId\"] = mid_enc.transform(ratings.movieId)\n","movies[\"movieId\"] = mid_enc.transform(movies.movieId)\n","\n","# Dictionary of movie id and title\n","mid_map = pd.Series(dict(zip(movies.movieId, movies.title)))\n","\n","# Number of users, number of movies\n","n_users, n_movies = len(uid_enc.classes_), len(mid_enc.classes_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lAwbYyNcITTE","colab_type":"text"},"cell_type":"markdown","source":["# Split Train, Test Data\n","* 以4分為閥值, 4分以上為positive, 未滿4分為negative\n","* 每個user分positive, negative兩部分, 各取30%到valid data"]},{"metadata":{"id":"rLB40Z00ITTF","colab_type":"code","colab":{}},"cell_type":"code","source":["def split_ratings(data, pos_thres=4, test_ratio=0.3):\n","    \"\"\"依照test_ratio切割movielens train test資料\"\"\"\n","    tr, te = [], []\n","    for u, df in data.groupby(\"userId\"):\n","        if len(df) < 5: continue\n","\n","        pos, neg = df.query(\"rating >= {}\".format(pos_thres)), df.query(\"rating < {}\".format(pos_thres))\n","        # Split positive part\n","        pos_len = int(len(pos) * (1 - test_ratio))\n","        tr_pos = pos[:pos_len]\n","        te_pos = pos[pos_len:]\n","        # Split negative part\n","        neg_len = int(len(neg) * (1 - test_ratio))\n","        tr_neg = neg[:neg_len]\n","        te_neg = neg[neg_len:]\n","\n","        tr.append(tr_pos.append(tr_neg))\n","        te.append(te_pos.append(te_neg))\n","    return pd.concat(tr, ignore_index=True), pd.concat(te, ignore_index=True)\n","\n","tr, te = split_ratings(ratings, 4, .3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nLh9NlnfITTH","colab_type":"text"},"cell_type":"markdown","source":["# Make Rating Matrix (Interaction Between Users and Movies)"]},{"metadata":{"id":"plMmmUmDITTI","colab_type":"code","colab":{}},"cell_type":"code","source":["tr_rating_mat = np.zeros((n_users, n_movies))\n","# Valid data rating matrix\n","te_rating_mat = np.zeros((n_users, n_movies))\n","\n","# Train rating matrix\n","for idx, r in tr.iterrows():\n","    tr_rating_mat[int(r.userId), int(r.movieId)] = r.rating\n","# Valid rating matrix    \n","for idx, r in te.iterrows():\n","    te_rating_mat[int(r.userId), int(r.movieId)] = r.rating\n","    \n","print('Shape of train interaction matrix: ', tr_rating_mat.shape)\n","print(tr_rating_mat, '\\n')\n","print('Shape of test interaction matrix: ', te_rating_mat.shape)\n","print(te_rating_mat)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g3KR-fNOITTL","colab_type":"text"},"cell_type":"markdown","source":["# Encode Movies Table"]},{"metadata":{"id":"ZpLaVwTsITTM","colab_type":"code","colab":{}},"cell_type":"code","source":["def do_movies(movies):\n","    movies = movies.reset_index(drop=True)\n","    movies[\"genres\"] = movies.genres.str.split(\"\\|\")\n","    genres_cnt = Counter()\n","    movies.genres.map(genres_cnt.update)\n","    genres_map = LabelEncoder()\n","    genres_map.fit( np.array(genres_cnt.most_common())[:, 0] )\n","    movies[\"genres\"] = movies.genres.map(lambda lst: genres_map.transform(lst))\n","    \n","    movies[\"avg_rating\"] = ratings.groupby(\"movieId\").rating.mean()\n","    movies[\"avg_rating\"] = scale(movies.avg_rating.fillna(movies.avg_rating.mean()))\n","    movies[\"freq_rating\"] = ratings.groupby(\"movieId\").size()\n","    movies[\"freq_rating\"] = scale(movies.avg_rating.fillna(movies.freq_rating.median()))\n","    movies[\"year\"] = movies.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n","    movies[\"year\"] = scale(movies.year.fillna(movies.year.mean()))\n","\n","    return movies, genres_map\n","\n","movies_encoded, genres_map = do_movies(movies)\n","n_genres = len(genres_map.classes_)\n","movies_encoded.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hGQSbZCfITTP","colab_type":"text"},"cell_type":"markdown","source":["# Encode Users Statistics"]},{"metadata":{"scrolled":true,"id":"s8Y5uYFvITTQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# user_encoded \n","user_encoded = ratings.groupby('userId').rating.agg(['size', 'mean'])\n","user_encoded.columns = ['user_rating_freq', 'user_rating_mean']\n","user_encoded['user_rating_freq'] = scale(user_encoded.user_rating_freq)\n","user_encoded['user_rating_mean'] = scale(user_encoded.user_rating_mean)\n","user_encoded = user_encoded.reset_index()\n","user_encoded.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OLGT50YNITTV","colab_type":"text"},"cell_type":"markdown","source":["# 以leave one out方式產生 train data, test data\n","1. 每一筆資料有兩部分: [user query] + [item id]\n","2. 每一筆user query 包含所有user movie history, 除了當前的rating movie (candidate movie)\n","3. test data的user query來自於train data"]},{"metadata":{"id":"sZ5GYjFtITTV","colab_type":"code","colab":{}},"cell_type":"code","source":["def loo_preprocess(data, movies_encoded, train_hist=None, is_train=True):\n","    \"\"\"以leave one out方式產生 train data, test data\"\"\"\n","    queue = []\n","    data = data.merge(movies_encoded, how=\"left\", on=\"movieId\")\n","    data = data.merge(user_encoded, how=\"left\", on=\"userId\")\n","    columns = [\"user_id\", \"query_movie_ids\", \"query_movie_ids_len\", \"user_rating_freq\", \"user_rating_mean\",\n","               \"genres\", \"genres_len\", \"avg_rating\", \"freq_rating\", \"year\", \"candidate_movie_id\",\n","               \"rating\"]\n","    \n","    for u, df in data.groupby(\"userId\"):\n","        df = df.sort_values(\"rating\", ascending=False)\n","        # 抓出user給予正向評價的電影 (>= 4)\n","        if is_train:\n","            fav_movies = set(df.query(\"rating >= 4\").movieId)\n","        else:\n","            fav_movies = set(train_hist.query(f\"userId == {u} and rating >= 4\").movieId)\n","        for i, (_, r) in enumerate(df.iterrows()):\n","            queries = list(fav_movies - set([int(r.movieId)]))\n","            # 對於multivalent的欄位, 需要增加一個column去描述該欄位的長度\n","            queue.append([int(r.userId),\n","                          queries,\n","                          len(queries),\n","                          r.user_rating_freq,\n","                          r.user_rating_mean,\n","                          r.genres,\n","                          len(r.genres),\n","                          r.avg_rating,\n","                          r.freq_rating,\n","                          r.year, \n","                          int(r.movieId), \n","                          r.rating])\n","    return pd.DataFrame(queue, columns=columns)\n","\n","trProcessed = loo_preprocess(tr, movies_encoded)\n","teProcessed = loo_preprocess(te, movies_encoded, tr, is_train=False)\n","trProcessed.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WnF9eJ1pITTa","colab_type":"code","colab":{}},"cell_type":"code","source":["teProcessed.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZDq73FZKITTd","colab_type":"text"},"cell_type":"markdown","source":["## Data Function\n","1. 由於 Keras(Tensorflow backend) 不支援變動長度的columns, 需透過padding zero(補零)帶入\n","2. 每個變動長度的column, 需要再給lens描述每一筆資料的長度, ex: query_movie_ids, query_movie_ids_len"]},{"metadata":{"scrolled":true,"id":"HLsct90sITTe","colab_type":"code","colab":{}},"cell_type":"code","source":["feats = [\"query_movie_ids\", \"query_movie_ids_len\", \"user_rating_freq\", \"user_rating_mean\",\n","         \"genres\", \"genres_len\", \"avg_rating\", \"freq_rating\", \"year\", \"candidate_movie_id\", 'global']\n","multi_cols = [\"query_movie_ids\", 'genres']\n","label = 'rating'\n","\n","# Generator function\n","def dataFn(data, n_batch=128, shuffle=False):\n","    pad = pad_sequences\n","    def fn():\n","        while True:\n","            dataInner = data.copy()\n","            indices = get_minibatches_idx(len(dataInner), n_batch, shuffle=shuffle)\n","            for ind in indices:\n","                ret = do_multi(dataInner.iloc[ind], multi_cols)\n","                ret['global'] = 0\n","                yield [np.stack(ret[col].values) if col in multi_cols else ret[col][:, None]\n","                       for col in feats], ret.rating.values[:, None]\n","    return fn\n","\n","def get_minibatches_idx(n, batch_size, shuffle=False):\n","    idx_list = np.arange(n, dtype=\"int32\")\n","    if shuffle:\n","        np.random.shuffle(idx_list)\n","    minibatches = []\n","    minibatch_start = 0\n","    for i in range(n // batch_size):\n","        minibatches.append(idx_list[minibatch_start : minibatch_start + batch_size])\n","        minibatch_start += batch_size\n","\n","    if (minibatch_start != n):\n","        # Make a minibatch out of what is left\n","        minibatches.append(idx_list[minibatch_start:])\n","    return minibatches\n","\n","def do_multi(df, multi_cols):\n","    \"\"\"Padding the multivalent feature\"\"\"\n","    pad = pad_sequences\n","    df = df.copy()\n","    for colname in multi_cols:\n","        lens = df[colname].map(len)\n","        df[colname] = list(pad(df[colname], padding=\"post\", maxlen=lens.max()))\n","    return df\n","\n","for data, label in dataFn(trProcessed, n_batch=5, shuffle=True)():\n","    break\n","\n","for name, col in zip(feats, data):\n","    print(f'{name}\\n{col}\\n')\n","print(f'label\\n{label}')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lp3sSdo8ITTg","colab_type":"code","colab":{}},"cell_type":"code","source":["tmp = pd.DataFrame()\n","for col, val in zip(feats, data):\n","    if col in multi_cols:\n","        tmp[col] = list(val)\n","    else:\n","        tmp[col] = val.ravel()\n","\n","tmp['rating'] = label.ravel()\n","tmp"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2pQ-Jfr1ITTk","colab_type":"text"},"cell_type":"markdown","source":["<br/>\n","<br/>\n","<br/>\n","<br/>\n","<br/>\n","<br/>"]},{"metadata":{"id":"BZ6TMIgFITTk","colab_type":"text"},"cell_type":"markdown","source":["# Model of Matrix Factorization with DNN"]},{"metadata":{"id":"UbYJfqpYITTl","colab_type":"text"},"cell_type":"markdown","source":["## Build Model Function"]},{"metadata":{"scrolled":true,"id":"T0PQH_HRITTm","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_model(n_users, n_movies, emb_size, reg):\n","    # Input tesors\n","    inp_query = Input([None], dtype='int32', name='inp_query')\n","    inp_query_len = Input([1], dtype='int32', name='inp_query_len')\n","    inp_u_freq = Input([1], dtype='float32', name='inp_u_freq')\n","    inp_u_mean = Input([1], dtype='float32', name='inp_u_mean')\n","    inp_genres = Input([None], dtype='int32', name='inp_genres')\n","    inp_genres_len = Input([1], dtype='int32', name='inp_genres_len')\n","    inp_avg_rating = Input([1], dtype='float32', name='inp_avg_rating')\n","    inp_freq_rating = Input([1], dtype='float32', name='inp_freq_rating')\n","    inp_year = Input([1], dtype='float32', name='inp_year')\n","    inp_movie = Input([1], dtype='int32', name='inp_movie')\n","    # Hack: only input integer => \"0\"\n","    inp_global = Input([1], dtype='int32', name='inp_global')\n","    \n","    # User, movie, genres embedding\n","    emb_query = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform', \n","                          embeddings_regularizer=regularizers.l2(reg))(inp_query)\n","    emb_genres = Embedding(n_genres, 8, embeddings_initializer='glorot_uniform',\n","                          embeddings_regularizer=regularizers.l2(reg))(inp_genres)\n","    emb_movie = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform',\n","                          embeddings_regularizer=regularizers.l2(reg))(inp_movie)\n","    \n","    # User side\n","    def sqrtn(x):\n","        qry, lens = x\n","        lens = tf.reshape(lens, [-1])\n","        weights = tf.nn.l2_normalize(tf.sequence_mask(lens, dtype=tf.float32), 1)\n","        weights = tf.expand_dims(weights, -1)\n","        return tf.reduce_sum(qry * weights, 1)\n","    emb_query = Lambda(sqrtn, name='emb_query')([emb_query, inp_query_len])\n","    emb_query = Concatenate(1)([emb_query, inp_u_freq, inp_u_mean])\n","    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_query)\n","    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='user_impression')(emb_query)\n","    \n","    # Movie side\n","    emb_genres = Lambda(sqrtn, name='emb_genres')([emb_genres, inp_genres_len])\n","    emb_movie = Flatten(name='emb_movie')(emb_movie)\n","    emb_movie = Concatenate(1)([emb_movie, emb_genres, inp_avg_rating, inp_freq_rating, inp_year])\n","    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_movie)\n","    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='movie_impression')(emb_movie)\n","    \n","    # Bias terms\n","    # Projection of emb_query to get bias\n","    b_user = Dense(1, \n","                   kernel_initializer='glorot_uniform',\n","                   kernel_regularizer=regularizers.l2(reg),\n","                   activation='linear', \n","                   use_bias=False,\n","                   name='b_user')(emb_query)\n","    # Projection of emb_movie to get bias\n","    b_movie = Dense(1, \n","                   kernel_initializer='glorot_uniform',\n","                   kernel_regularizer=regularizers.l2(reg),\n","                   activation='linear', \n","                   use_bias=False,\n","                   name='b_movie')(emb_movie)\n","    b_global = Flatten(name='b_global')(Embedding(1, 1, embeddings_initializer='glorot_uniform')(inp_global))\n","    \n","    # Implements the formulation\n","    nets = dot([emb_query, emb_movie], axes=1)\n","    nets = add([nets, b_user, b_movie, b_global])\n","    \n","    model = Model([inp_query, \n","                   inp_query_len, \n","                   inp_u_freq,\n","                   inp_u_mean,\n","                   inp_genres, \n","                   inp_genres_len, \n","                   inp_avg_rating,\n","                   inp_freq_rating,\n","                   inp_year,\n","                   inp_movie, \n","                   inp_global], nets)\n","    model.summary()\n","    return model, Model([inp_movie, \n","                         inp_genres, \n","                         inp_genres_len, \n","                         inp_avg_rating,\n","                         inp_freq_rating,\n","                         inp_year], emb_movie)\n","\n","emb_size = 8\n","reg = 0.0005\n","batch_size = 128\n","epochs = 10\n","\n","K.clear_session()\n","model_mf_dnn, model_emb_movie = get_model(n_users, n_movies, emb_size, reg)\n","model_mf_dnn.compile(optimizer=SGD(lr=0.05), loss='mse')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kuMa62RwITTp","colab_type":"text"},"cell_type":"markdown","source":["# Training"]},{"metadata":{"id":"8rH_vMs3ITTq","colab_type":"text"},"cell_type":"markdown","source":["## Use Callback Function\n","* keras.callbacks.ModelCheckpoint: 只存檔最好的結果, 是另一種防止overfitting的方式\n","    * save_best_only = True"]},{"metadata":{"scrolled":false,"id":"dA9S8yPnITTr","colab_type":"code","colab":{}},"cell_type":"code","source":["model_dir = \"./model_mf_dnn\"\n","\n","tr_len = len(trProcessed)\n","te_len = len(teProcessed)\n","hist = model_mf_dnn.fit_generator(\n","    generator=dataFn(trProcessed, n_batch=batch_size, shuffle=True)(),\n","    steps_per_epoch=tr_len // batch_size + (1 if tr_len % batch_size else 0),\n","    validation_data=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n","    validation_steps=te_len // batch_size + (1 if te_len % batch_size else 0),\n","    # batch_size=batch_size,\n","    epochs=epochs,\n","    callbacks=[ModelCheckpoint(filepath=model_dir, \n","                               save_weights_only=True, \n","                               save_best_only=True)]\n",")\n","\n","# After training, load the best weights back\n","model_mf_dnn.load_weights(model_dir)\n","\n","sns.lineplot(np.arange(len(hist.history['loss'])), hist.history['loss'], label='train')\n","sns.lineplot(np.arange(len(hist.history['val_loss'])), hist.history['val_loss'], label='test')\n","plt.title('loss')\n","plt.grid(True)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k3icFptzITTs","colab_type":"text"},"cell_type":"markdown","source":["# Prediction"]},{"metadata":{"id":"lLtz1c9gITTs","colab_type":"code","colab":{}},"cell_type":"code","source":["te_len = len(teProcessed)\n","pred = model_mf_dnn.predict_generator(\n","    generator=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n","    steps=te_len // batch_size + (1 if te_len % batch_size else 0)\n",").ravel()\n","print('Shape of test data: ', pred.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gGcZRFeUITTv","colab_type":"text"},"cell_type":"markdown","source":["# Metrics\n","* 定義4分以上為正向評價, 4分以下為負向評價"]},{"metadata":{"id":"IT13YnBMITTw","colab_type":"text"},"cell_type":"markdown","source":["## RMSE "]},{"metadata":{"scrolled":true,"id":"8cOGf19vITTx","colab_type":"code","colab":{}},"cell_type":"code","source":["te_len = len(teProcessed)\n","valis_steps = te_len // batch_size + (1 if te_len % batch_size else 0)\n","\n","te_y = []\n","for i, (feat, label) in enumerate(dataFn(teProcessed, \n","                                         n_batch=batch_size, \n","                                         shuffle=False)(), 1):\n","    if i > valis_steps: break\n","    te_y += label.ravel().tolist()\n","    \n","te_y = np.array(te_y)\n","print(\"RMSE: \", np.sqrt(np.mean((pred - te_y)**2)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eomC8oPCITTz","colab_type":"text"},"cell_type":"markdown","source":["## AUC"]},{"metadata":{"id":"g2-sErYaITT0","colab_type":"code","colab":{}},"cell_type":"code","source":["def draw_roc_curve(y, pred_proba):\n","    fpr, tpr, _ = roc_curve(y, pred_proba, pos_label=1)\n","    auc_scr = auc(fpr, tpr)\n","    print(\"auc:\", auc_scr)\n","    f, ax = plt.subplots(1, 1, figsize=(6, 6))\n","\n","    ax.plot([0, 1], [0, 1], 'k--')\n","    ax.plot(fpr, tpr, label='ROC CURVE')\n","    ax.set_xlabel('False positive rate')\n","    ax.set_ylabel('True positive rate')\n","    ax.set_title('Area Under Curve(ROC) (score: {:.4f})'.format(auc_scr))\n","    ax.legend(loc='best')\n","    plt.grid(True)\n","    plt.show()\n","    \n","draw_roc_curve(te.rating >= 4, pred / pred.max())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fjNLQrCYITT3","colab_type":"text"},"cell_type":"markdown","source":["## Single User Rating Histogram"]},{"metadata":{"scrolled":true,"id":"YKEv6ZgHITT5","colab_type":"code","colab":{}},"cell_type":"code","source":["# user id from 0 ~ 670\n","uid = 22\n","tmp = teProcessed.query(f\"user_id == {uid}\")\n","single_pred = model_mf_dnn.predict_generator(\n","    generator=dataFn(tmp, n_batch=batch_size, shuffle=False)(),\n","    steps=len(tmp) // batch_size + (1 if len(tmp) % batch_size else 0)\n",").ravel()\n","\n","f, ax = plt.subplots(1, 2, figsize=(10, 5))\n","ax[0].set_title(\"pred distribute\")\n","sns.distplot(single_pred, ax=ax[0])\n","ax[1].set_title(\"real distribute\")\n","sns.distplot(te.query(f\"userId == '{uid}'\").rating, ax=ax[1])\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l5DpJZcaITT8","colab_type":"text"},"cell_type":"markdown","source":["## Single User Detail Table"]},{"metadata":{"scrolled":true,"id":"S8bNNaT5ITT9","colab_type":"code","colab":{}},"cell_type":"code","source":["# user id from 0 ~ 670\n","uid = 22\n","tmp = teProcessed.query(f\"user_id == {uid}\")\n","single_pred = model_mf_dnn.predict_generator(\n","    generator=dataFn(tmp, n_batch=batch_size, shuffle=False)(),\n","    steps=len(tmp) // batch_size + (1 if len(tmp) % batch_size else 0)\n",").ravel()\n","\n","recommDf = pd.DataFrame(data={\n","              \"userId\": uid,\n","              \"movieId\": tmp.candidate_movie_id,\n","              \"title\": mid_map[tmp.candidate_movie_id].values,\n","              \"rating\": tmp.rating.values,\n","              \"predRating\": single_pred},\n","             columns=(\"userId\", \"movieId\", \"title\", \"rating\", \"predRating\"))\n","# ascending 可以調整True or False觀察結果\n","recommDf.sort_values(\"rating\", ascending=False)"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"hpiqgPKGITT_","colab_type":"code","colab":{}},"cell_type":"code","source":["recommDf.sort_values(\"predRating\", ascending=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hQgIzaZ4ITUC","colab_type":"text"},"cell_type":"markdown","source":["<br/>\n","<br/>\n","<br/>\n","\n","# 利用 Movie Embedding 找出相似電影"]},{"metadata":{"id":"0Snfb5OUITUC","colab_type":"code","colab":{}},"cell_type":"code","source":["movies[movies.title.str.contains(\"Toy\")]"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"Xza_nN49ITUF","colab_type":"code","colab":{}},"cell_type":"code","source":["model_mf_dnn.load_weights(model_dir)\n","\n","# Movie data function generator\n","movies_cols = ['movieId', 'genres', 'genres_len', 'avg_rating', 'freq_rating', 'year']\n","def movie_data_fn(data, batch_size=128):\n","    def _fn():\n","        data_inner = data.copy()\n","        while True:\n","            indices = get_minibatches_idx(len(data_inner), batch_size, shuffle=False)\n","            for ind in indices:\n","                ret = do_multi(data_inner.iloc[ind], ['genres'])\n","                ret['global'] = 0\n","                yield [np.stack(ret[col].values) if col in ['genres'] else ret[col][:, None]\n","                       for col in movies_cols]\n","    return _fn\n","\n","def most_like(model, seed_movie, k=10):\n","    \"\"\"給定某一部電影, 使用model裡movies embedding找尋cosine相似度高的其他電影!\"\"\"\n","    tmp = movies_encoded.copy()\n","    tmp['genres_len'] = tmp.genres.map(len)\n","    movie_emb = model.predict_generator(\n","        generator=movie_data_fn(tmp, batch_size)(),\n","        steps=len(tmp) // batch_size + (1 if len(tmp) % batch_size else 0)\n","    )\n","    # print(cosine_similarity(movie_emb[seed_movie][np.newaxis, :], movie_emb))\n","    most_like = cosine_similarity(movie_emb[seed_movie][np.newaxis, :], movie_emb).ravel().argsort()[::-1][:k]\n","    return movies.iloc[most_like]\n","\n","# mse訓練出來的model\n","most_like(model_emb_movie, 7575, k=11)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Iz7Y1uDfITUI","colab_type":"text"},"cell_type":"markdown","source":["<br/>\n","<br/>\n","<br/>\n","\n","# (LAB) 將Model從Regression改為Classification"]},{"metadata":{"id":"KLjmDNf6ITUJ","colab_type":"text"},"cell_type":"markdown","source":["## Modify Data Generator"]},{"metadata":{"scrolled":true,"id":"3JSFhNpGITUJ","colab_type":"code","colab":{}},"cell_type":"code","source":["# Generator function\n","def dataFn(data, n_batch=128, shuffle=False):\n","    pad = pad_sequences\n","    def fn():\n","        while True:\n","            dataInner = data.copy()\n","            indices = get_minibatches_idx(len(dataInner), n_batch, shuffle=shuffle)\n","            for ind in indices:\n","                ret = do_multi(dataInner.iloc[ind], multi_cols)\n","                ret['global'] = 0\n","                yield [np.stack(ret[col].values) if col in multi_cols else ret[col][:, None]\n","                       for col in feats], (ret.rating >= 4).astype(int)[:, None]\n","    return fn\n","\n","for data, label in dataFn(trProcessed, n_batch=5, shuffle=False)():\n","    break\n","\n","print(f'label\\n{label}')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y9nZQGkKITUM","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_model(n_users, n_movies, emb_size, reg):\n","    # Input tesors\n","    inp_query = Input([None], dtype='int32', name='inp_query')\n","    inp_query_len = Input([1], dtype='int32', name='inp_query_len')\n","    inp_u_freq = Input([1], dtype='float32', name='inp_u_freq')\n","    inp_u_mean = Input([1], dtype='float32', name='inp_u_mean')\n","    inp_genres = Input([None], dtype='int32', name='inp_genres')\n","    inp_genres_len = Input([1], dtype='int32', name='inp_genres_len')\n","    inp_avg_rating = Input([1], dtype='float32', name='inp_avg_rating')\n","    inp_freq_rating = Input([1], dtype='float32', name='inp_freq_rating')\n","    inp_year = Input([1], dtype='float32', name='inp_year')\n","    inp_movie = Input([1], dtype='int32', name='inp_movie')\n","    # Hack: only input integer => \"0\"\n","    inp_global = Input([1], dtype='int32', name='inp_global')\n","    \n","    # User, movie, genres embedding\n","    emb_query = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform', \n","                          embeddings_regularizer=regularizers.l2(reg))(inp_query)\n","    emb_genres = Embedding(n_genres, 8, embeddings_initializer='glorot_uniform',\n","                          embeddings_regularizer=regularizers.l2(reg))(inp_genres)\n","    emb_movie = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform',\n","                          embeddings_regularizer=regularizers.l2(reg))(inp_movie)\n","    \n","    # User side\n","    def sqrtn(x):\n","        qry, lens = x\n","        lens = tf.reshape(lens, [-1])\n","        weights = tf.nn.l2_normalize(tf.sequence_mask(lens, dtype=tf.float32), 1)\n","        weights = tf.expand_dims(weights, -1)\n","        return tf.reduce_sum(qry * weights, 1)\n","    emb_query = Lambda(sqrtn, name='emb_query')([emb_query, inp_query_len])\n","    emb_query = Concatenate(1)([emb_query, inp_u_freq, inp_u_mean])\n","    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_query)\n","    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='user_impression')(emb_query)\n","    \n","    # Movie side\n","    emb_genres = Lambda(sqrtn, name='emb_genres')([emb_genres, inp_genres_len])\n","    emb_movie = Flatten(name='emb_movie')(emb_movie)\n","    emb_movie = Concatenate(1)([emb_movie, emb_genres, inp_avg_rating, inp_freq_rating, inp_year])\n","    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_movie)\n","    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='movie_impression')(emb_movie)\n","    \n","    # Bias terms\n","    # Projection of emb_query to get bias\n","    b_user = Dense(1, \n","                   kernel_initializer='glorot_uniform',\n","                   kernel_regularizer=regularizers.l2(reg),\n","                   activation='linear', \n","                   use_bias=False,\n","                   name='b_user')(emb_query)\n","    # Projection of emb_movie to get bias\n","    b_movie = Dense(1, \n","                   kernel_initializer='glorot_uniform',\n","                   kernel_regularizer=regularizers.l2(reg),\n","                   activation='linear', \n","                   use_bias=False,\n","                   name='b_movie')(emb_movie)\n","    b_global = Flatten(name='b_global')(Embedding(1, 1, embeddings_initializer='glorot_uniform')(inp_global))\n","    \n","    # Implements the formulation\n","    nets = dot([emb_query, emb_movie], axes=1)\n","    nets = add([nets, b_user, b_movie, b_global])\n","    \n","    ###### START CODE HERE ######\n","    # Modify the model prediction to 0 ~ 1, hint: add an activation function\n","    # ...\n","    ###### END CODE HERE ######\n","    \n","    model = Model([inp_query, \n","                   inp_query_len, \n","                   inp_u_freq,\n","                   inp_u_mean,\n","                   inp_genres, \n","                   inp_genres_len, \n","                   inp_avg_rating,\n","                   inp_freq_rating,\n","                   inp_year,\n","                   inp_movie, \n","                   inp_global], nets)\n","    model.summary()\n","    return model, Model([inp_movie, \n","                         inp_genres, \n","                         inp_genres_len, \n","                         inp_avg_rating,\n","                         inp_freq_rating,\n","                         inp_year], emb_movie)\n","\n","###### START CODE HERE ######\n","# Modify the hyper parameters to get even better result\n","emb_size = # 8, 10, 16 ...\n","reg = # 0.01, 0.005, 0.0005 ...\n","batch_size = 128\n","epochs =  # 10, 20 , 30 ...\n","lr = # 0.1, 0.05, 0.001\n","###### END CODE HERE ######\n","\n","model_dir = \"./model_mf_dnn\"\n","K.clear_session()\n","model_mf_dnn, model_emb_movie = get_model(n_users, n_movies, emb_size, reg)\n","\n","###### START CODE HERE ######\n","# Find best optimizer, e.g: Adam, SGD, Adagrad, find proper loss function\n","# model_mf_dnn.compile(...)\n","###### END CODE HERE ######\n","\n","tr_len = len(trProcessed)\n","te_len = len(teProcessed)\n","hist = model_mf_dnn.fit_generator(\n","    generator=dataFn(trProcessed, n_batch=batch_size, shuffle=True)(),\n","    steps_per_epoch=tr_len // batch_size + (1 if tr_len % batch_size else 0),\n","    validation_data=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n","    validation_steps=te_len // batch_size + (1 if te_len % batch_size else 0),\n","    # batch_size=batch_size,\n","    epochs=epochs,\n","    callbacks=[ModelCheckpoint(filepath=model_dir, \n","                               save_weights_only=True, \n","                               save_best_only=True)]\n",")\n","\n","# After training, load the best weights back\n","model_mf_dnn.load_weights(model_dir)\n","\n","sns.lineplot(np.arange(len(hist.history['loss'])), hist.history['loss'], label='train')\n","sns.lineplot(np.arange(len(hist.history['val_loss'])), hist.history['val_loss'], label='test')\n","plt.title('loss')\n","plt.grid(True)\n","plt.show()\n","\n","# Prediction\n","pred = model_mf_dnn.predict_generator(\n","    generator=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n","    steps=te_len // batch_size + (1 if te_len % batch_size else 0)\n",").ravel()\n","\n","\n","te_labels = (teProcessed.rating >= 4).astype(int)\n","\n","# AUC\n","print('Shape of test data: ', pred.shape)\n","draw_roc_curve(te_labels, pred)\n","\n","# Confusion matrix, classification report\n","from sklearn.metrics import confusion_matrix, classification_report\n","print('accuracy_score: ', accuracy_score(te_labels, pred >= 0.5))\n","print(confusion_matrix(te_labels, pred >= 0.5))\n","print()\n","print(classification_report(te_labels, pred >= 0.5))\n","\n","most_like(model_emb_movie, 8787, k=11)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"juY1cFwvITUO","colab_type":"text"},"cell_type":"markdown","source":["## 利用Movie Embedding, 以Cosine Similarity找出前10名相似電影"]},{"metadata":{"id":"z3UXF7-nITUO","colab_type":"code","colab":{}},"cell_type":"code","source":["movies[movies.title.str.contains(\"Inception\")]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vPA3TWFGITUQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# Call most_like function 找出前10名相似電影\n","# most_like(...)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8uzQ0kyeITUS","colab_type":"text"},"cell_type":"markdown","source":["<br/>\n","<br/>\n","<br/>\n","<br/>\n","<br/>\n","<br/>"]},{"metadata":{"id":"zUrdU2WyITUT","colab_type":"text"},"cell_type":"markdown","source":["## Solution"]},{"metadata":{"scrolled":true,"id":"f0sPoeChITUW","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_model(n_users, n_movies, emb_size, reg):\n","    # Input tesors\n","    inp_query = Input([None], dtype='int32', name='inp_query')\n","    inp_query_len = Input([1], dtype='int32', name='inp_query_len')\n","    inp_u_freq = Input([1], dtype='float32', name='inp_u_freq')\n","    inp_u_mean = Input([1], dtype='float32', name='inp_u_mean')\n","    inp_genres = Input([None], dtype='int32', name='inp_genres')\n","    inp_genres_len = Input([1], dtype='int32', name='inp_genres_len')\n","    inp_avg_rating = Input([1], dtype='float32', name='inp_avg_rating')\n","    inp_freq_rating = Input([1], dtype='float32', name='inp_freq_rating')\n","    inp_year = Input([1], dtype='float32', name='inp_year')\n","    inp_movie = Input([1], dtype='int32', name='inp_movie')\n","    # Hack: only input integer => \"0\"\n","    inp_global = Input([1], dtype='int32', name='inp_global')\n","    \n","    # User, movie, genres embedding\n","    emb_query = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform', \n","                          embeddings_regularizer=regularizers.l2(reg))(inp_query)\n","    emb_genres = Embedding(n_genres, 8, embeddings_initializer='glorot_uniform',\n","                          embeddings_regularizer=regularizers.l2(reg))(inp_genres)\n","    emb_movie = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform',\n","                          embeddings_regularizer=regularizers.l2(reg))(inp_movie)\n","    \n","    # User side\n","    def sqrtn(x):\n","        qry, lens = x\n","        lens = tf.reshape(lens, [-1])\n","        weights = tf.nn.l2_normalize(tf.sequence_mask(lens, dtype=tf.float32), 1)\n","        weights = tf.expand_dims(weights, -1)\n","        return tf.reduce_sum(qry * weights, 1)\n","    emb_query = Lambda(sqrtn, name='emb_query')([emb_query, inp_query_len])\n","    emb_query = Concatenate(1)([emb_query, inp_u_freq, inp_u_mean])\n","    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_query)\n","    emb_query = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='user_impression')(emb_query)\n","    \n","    # Movie side\n","    emb_genres = Lambda(sqrtn, name='emb_genres')([emb_genres, inp_genres_len])\n","    emb_movie = Flatten(name='emb_movie')(emb_movie)\n","    emb_movie = Concatenate(1)([emb_movie, emb_genres, inp_avg_rating, inp_freq_rating, inp_year])\n","    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg))(emb_movie)\n","    emb_movie = Dense(emb_size, activation='relu', kernel_regularizer=regularizers.l2(reg), name='movie_impression')(emb_movie)\n","    \n","    # Bias terms\n","    # Projection of emb_query to get bias\n","    b_user = Dense(1, \n","                   kernel_initializer='glorot_uniform',\n","                   kernel_regularizer=regularizers.l2(reg),\n","                   activation='linear', \n","                   use_bias=False,\n","                   name='b_user')(emb_query)\n","    # Projection of emb_movie to get bias\n","    b_movie = Dense(1, \n","                   kernel_initializer='glorot_uniform',\n","                   kernel_regularizer=regularizers.l2(reg),\n","                   activation='linear', \n","                   use_bias=False,\n","                   name='b_movie')(emb_movie)\n","    b_global = Flatten(name='b_global')(Embedding(1, 1, embeddings_initializer='glorot_uniform')(inp_global))\n","    \n","    # Implements the formulation\n","    nets = dot([emb_query, emb_movie], axes=1)\n","    nets = add([nets, b_user, b_movie, b_global])\n","    \n","    ###### START CODE HERE ######\n","    # Modify the model prediction to 0 ~ 1, hint: add an activation function\n","    nets = Activation(\"sigmoid\")(nets)\n","    ###### END CODE HERE ######\n","    \n","    model = Model([inp_query, \n","                   inp_query_len, \n","                   inp_u_freq,\n","                   inp_u_mean,\n","                   inp_genres, \n","                   inp_genres_len, \n","                   inp_avg_rating,\n","                   inp_freq_rating,\n","                   inp_year,\n","                   inp_movie, \n","                   inp_global], nets)\n","    model.summary()\n","    return model, Model([inp_movie, \n","                         inp_genres, \n","                         inp_genres_len, \n","                         inp_avg_rating,\n","                         inp_freq_rating,\n","                         inp_year], emb_movie)\n","\n","###### START CODE HERE ######\n","# Modify the hyper parameters to get even better result\n","emb_size = 16\n","reg = 0.0005\n","batch_size = 128\n","epochs = 10\n","lr = 0.05\n","###### END CODE HERE ######\n","\n","model_dir = \"./model_mf_dnn\"\n","K.clear_session()\n","model_mf_dnn, model_emb_movie = get_model(n_users, n_movies, emb_size, reg)\n","\n","model_mf_dnn.compile(optimizer=SGD(lr=lr), loss='binary_crossentropy')\n","\n","tr_len = len(trProcessed)\n","te_len = len(teProcessed)\n","hist = model_mf_dnn.fit_generator(\n","    generator=dataFn(trProcessed, n_batch=batch_size, shuffle=True)(),\n","    steps_per_epoch=tr_len // batch_size + (1 if tr_len % batch_size else 0),\n","    validation_data=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n","    validation_steps=te_len // batch_size + (1 if te_len % batch_size else 0),\n","    # batch_size=batch_size,\n","    epochs=epochs,\n","    callbacks=[ModelCheckpoint(filepath=model_dir, \n","                               save_weights_only=True, \n","                               save_best_only=True)]\n",")\n","\n","# After training, load the best weights back\n","model_mf_dnn.load_weights(model_dir)\n","\n","sns.lineplot(np.arange(len(hist.history['loss'])), hist.history['loss'], label='train')\n","sns.lineplot(np.arange(len(hist.history['val_loss'])), hist.history['val_loss'], label='test')\n","plt.title('loss')\n","plt.grid(True)\n","plt.show()\n","\n","# Prediction\n","pred = model_mf_dnn.predict_generator(\n","    generator=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n","    steps=te_len // batch_size + (1 if te_len % batch_size else 0)\n",").ravel()\n","\n","te_labels = (teProcessed.rating >= 4).astype(int)\n","# AUC\n","print('Shape of test data: ', pred.shape)\n","draw_roc_curve(te_labels, pred)\n","\n","# Confusion matrix, classification report\n","print('accuracy_score: ', accuracy_score(te_labels, pred >= 0.5))\n","print(confusion_matrix(te_labels, pred >= 0.5))\n","print()\n","print(classification_report(te_labels, pred >= 0.5))\n","\n","most_like(model_emb_movie, 0, k=11)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-hWhb52yITUZ","colab_type":"text"},"cell_type":"markdown","source":["<br/>\n","<br/>\n","<br/>\n","\n","# (LAB) 延續Classification, 以DNN作法取代MF作法\n","\n","* Concatenate [user, movie], 且用 dense layer 增加hidden layers"]},{"metadata":{"id":"EZ87JqpYITUZ","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_model(n_users, n_movies, emb_size, reg):\n","    # Input tesors\n","    inp_query = Input([None], dtype='int32', name='inp_query')\n","    inp_query_len = Input([1], dtype='int32', name='inp_query_len')\n","    inp_u_freq = Input([1], dtype='float32', name='inp_u_freq')\n","    inp_u_mean = Input([1], dtype='float32', name='inp_u_mean')\n","    inp_genres = Input([None], dtype='int32', name='inp_genres')\n","    inp_genres_len = Input([1], dtype='int32', name='inp_genres_len')\n","    inp_avg_rating = Input([1], dtype='float32', name='inp_avg_rating')\n","    inp_freq_rating = Input([1], dtype='float32', name='inp_freq_rating')\n","    inp_year = Input([1], dtype='float32', name='inp_year')\n","    inp_movie = Input([1], dtype='int32', name='inp_movie')\n","    # Hack: only input integer => \"0\"\n","    inp_global = Input([1], dtype='int32', name='inp_global')\n","    \n","    # User, movie, genres embedding\n","    emb_query = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform', \n","                          embeddings_regularizer=regularizers.l2(reg))(inp_query)\n","    emb_genres = Embedding(n_genres, 8, embeddings_initializer='glorot_uniform',\n","                          embeddings_regularizer=regularizers.l2(reg))(inp_genres)\n","    emb_movie = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform',\n","                          embeddings_regularizer=regularizers.l2(reg))(inp_movie)\n","    \n","    # User side\n","    def sqrtn(x):\n","        qry, lens = x\n","        lens = tf.reshape(lens, [-1])\n","        weights = tf.nn.l2_normalize(tf.sequence_mask(lens, dtype=tf.float32), 1)\n","        weights = tf.expand_dims(weights, -1)\n","        return tf.reduce_sum(qry * weights, 1)\n","    emb_query = Lambda(sqrtn, name='emb_query')([emb_query, inp_query_len])\n","    emb_query = Concatenate(1)([emb_query, inp_u_freq, inp_u_mean])\n","    emb_query = Dense(emb_size, activation='relu')(emb_query)\n","    emb_query = Dense(emb_size, activation='relu', name='user_impression')(emb_query)\n","    \n","    # Movie side\n","    emb_genres = Lambda(sqrtn, name='emb_genres')([emb_genres, inp_genres_len])\n","    emb_movie = Flatten(name='emb_movie')(emb_movie)\n","    emb_movie = Concatenate(1)([emb_movie, emb_genres, inp_avg_rating, inp_freq_rating, inp_year])\n","    emb_movie = Dense(emb_size, activation='relu')(emb_movie)\n","    emb_movie = Dense(emb_size, activation='relu', name='movie_impression')(emb_movie)\n","    \n","    ###### START CODE HERE ######\n","    # Concatenate the user and movie embedding, add 2 hidden layers\n","    # Maybe you need Dropout! BatchNormalization, etc..., try it for the better result!\n","    # nets = Concatenate(1)([emb_query, emb_movie])\n","    # nets = Dense(...)(nets)\n","    # ...\n","    ###### END CODE HERE ######\n","    \n","    model = Model([inp_query, \n","                   inp_query_len, \n","                   inp_u_freq,\n","                   inp_u_mean,\n","                   inp_genres, \n","                   inp_genres_len, \n","                   inp_avg_rating,\n","                   inp_freq_rating,\n","                   inp_year,\n","                   inp_movie, \n","                   inp_global], nets)\n","    model.summary()\n","    return model, Model([inp_movie, \n","                         inp_genres, \n","                         inp_genres_len, \n","                         inp_avg_rating,\n","                         inp_freq_rating,\n","                         inp_year], emb_movie)\n","\n","###### START CODE HERE ######\n","# Modify the hyper parameters to get even better result\n","emb_size = # 8, 10, 16 ...\n","reg = # 0.01, 0.005, 0.0005 ...\n","batch_size = 128\n","epochs =  # 10, 20 , 30 ...\n","lr = # 0.1, 0.05, 0.001\n","###### END CODE HERE ######\n","\n","model_dir = \"./model_mf_dnn\"\n","K.clear_session()\n","model_mf_dnn, model_emb_movie = get_model(n_users, n_movies, emb_size, reg)\n","\n","###### START CODE HERE ######\n","# Find best optimizer, e.g: Adam, SGD, Adagrad, find proper loss function\n","# model_mf.compile(...)\n","###### END CODE HERE ######\n","\n","tr_len = len(trProcessed)\n","te_len = len(teProcessed)\n","hist = model_mf_dnn.fit_generator(\n","    generator=dataFn(trProcessed, n_batch=batch_size, shuffle=True)(),\n","    steps_per_epoch=tr_len // batch_size + (1 if tr_len % batch_size else 0),\n","    validation_data=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n","    validation_steps=te_len // batch_size + (1 if te_len % batch_size else 0),\n","    # batch_size=batch_size,\n","    epochs=epochs,\n","    callbacks=[ModelCheckpoint(filepath=model_dir, \n","                               save_weights_only=True, \n","                               save_best_only=True)]\n",")\n","\n","# After training, load the best weights back\n","model_mf_dnn.load_weights(model_dir)\n","\n","sns.lineplot(np.arange(len(hist.history['loss'])), hist.history['loss'], label='train')\n","sns.lineplot(np.arange(len(hist.history['val_loss'])), hist.history['val_loss'], label='test')\n","plt.title('loss')\n","plt.grid(True)\n","plt.show()\n","\n","# Prediction\n","te_len = len(teProcessed)\n","pred = model_mf_dnn.predict_generator(\n","    generator=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n","    steps=te_len // batch_size + (1 if te_len % batch_size else 0)\n",").ravel()\n","\n","\n","te_labels = (teProcessed.rating >= 4).astype(int)\n","# AUC\n","print('Shape of test data: ', pred.shape)\n","draw_roc_curve(te_labels, pred)\n","\n","# Confusion matrix, classification report\n","print('accuracy_score: ', accuracy_score(te_labels, pred >= 0.5))\n","print(confusion_matrix(te_labels, pred >= 0.5))\n","print()\n","print(classification_report(te_labels, pred >= 0.5))\n","\n","most_like(model_emb_movie, 8787, k=11)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rrN1s42DITUc","colab_type":"text"},"cell_type":"markdown","source":["<br/>\n","<br/>\n","<br/>\n","<br/>\n","<br/>\n","<br/>"]},{"metadata":{"id":"HICHEu6jITUc","colab_type":"text"},"cell_type":"markdown","source":["## Solution"]},{"metadata":{"scrolled":true,"id":"B0E5CyPWITUd","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_model(n_users, n_movies, emb_size, reg):\n","    # Input tesors\n","    inp_query = Input([None], dtype='int32', name='inp_query')\n","    inp_query_len = Input([1], dtype='int32', name='inp_query_len')\n","    inp_u_freq = Input([1], dtype='float32', name='inp_u_freq')\n","    inp_u_mean = Input([1], dtype='float32', name='inp_u_mean')\n","    inp_genres = Input([None], dtype='int32', name='inp_genres')\n","    inp_genres_len = Input([1], dtype='int32', name='inp_genres_len')\n","    inp_avg_rating = Input([1], dtype='float32', name='inp_avg_rating')\n","    inp_freq_rating = Input([1], dtype='float32', name='inp_freq_rating')\n","    inp_year = Input([1], dtype='float32', name='inp_year')\n","    inp_movie = Input([1], dtype='int32', name='inp_movie')\n","    # Hack: only input integer => \"0\"\n","    inp_global = Input([1], dtype='int32', name='inp_global')\n","    \n","    # User, movie, genres embedding\n","    emb_query = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform', \n","                          embeddings_regularizer=regularizers.l2(reg))(inp_query)\n","    emb_genres = Embedding(n_genres, 8, embeddings_initializer='glorot_uniform',\n","                          embeddings_regularizer=regularizers.l2(reg))(inp_genres)\n","    emb_movie = Embedding(n_movies, emb_size, embeddings_initializer='glorot_uniform',\n","                          embeddings_regularizer=regularizers.l2(reg))(inp_movie)\n","    \n","    # User side\n","    def sqrtn(x):\n","        qry, lens = x\n","        lens = tf.reshape(lens, [-1])\n","        weights = tf.nn.l2_normalize(tf.sequence_mask(lens, dtype=tf.float32), 1)\n","        weights = tf.expand_dims(weights, -1)\n","        return tf.reduce_sum(qry * weights, 1)\n","    emb_query = Lambda(sqrtn, name='emb_query')([emb_query, inp_query_len])\n","    emb_query = Concatenate(1)([emb_query, inp_u_freq, inp_u_mean])\n","    emb_query = Dense(emb_size, activation='relu')(emb_query)\n","    emb_query = Dense(emb_size, activation='relu', name='user_impression')(emb_query)\n","    \n","    # Movie side\n","    emb_genres = Lambda(sqrtn, name='emb_genres')([emb_genres, inp_genres_len])\n","    emb_movie = Flatten(name='emb_movie')(emb_movie)\n","    emb_movie = Concatenate(1)([emb_movie, emb_genres, inp_avg_rating, inp_freq_rating, inp_year])\n","    emb_movie = Dense(emb_size, activation='relu')(emb_movie)\n","    emb_movie = Dense(emb_size, activation='relu', name='movie_impression')(emb_movie)\n","    \n","    ###### START CODE HERE ######\n","    # Concatenate the user and movie embedding, add 2 hidden layers\n","    # Maybe you need Dropout! BatchNormalization, etc..., try it for the better result!\n","    nets = Concatenate(1)([emb_query, emb_movie])\n","    nets = Dense(32, activation='linear')(nets)\n","    nets = Activation('relu')(BatchNormalization()(nets))\n","    nets = Dense(16, activation='linear')(nets)\n","    nets = Activation('relu')(BatchNormalization()(nets))\n","    nets = Dense(1, activation='sigmoid')(nets)\n","    ###### END CODE HERE ######\n","    \n","    model = Model([inp_query, \n","                   inp_query_len, \n","                   inp_u_freq,\n","                   inp_u_mean,\n","                   inp_genres, \n","                   inp_genres_len, \n","                   inp_avg_rating,\n","                   inp_freq_rating,\n","                   inp_year,\n","                   inp_movie, \n","                   inp_global], nets)\n","    model.summary()\n","    return model, Model([inp_movie, \n","                         inp_genres, \n","                         inp_genres_len, \n","                         inp_avg_rating,\n","                         inp_freq_rating,\n","                         inp_year], emb_movie)\n","\n","###### START CODE HERE ######\n","# Modify the hyper parameters to get even better result\n","emb_size = 16\n","reg = 0.0005\n","batch_size = 128\n","epochs = 10\n","lr = 0.05\n","###### END CODE HERE ######\n","\n","model_dir = \"./model_mf_dnn\"\n","K.clear_session()\n","model_mf_dnn, model_emb_movie = get_model(n_users, n_movies, emb_size, reg)\n","\n","model_mf_dnn.compile(optimizer=SGD(lr=lr), loss='binary_crossentropy')\n","\n","tr_len = len(trProcessed)\n","te_len = len(teProcessed)\n","hist = model_mf_dnn.fit_generator(\n","    generator=dataFn(trProcessed, n_batch=batch_size, shuffle=True)(),\n","    steps_per_epoch=tr_len // batch_size + (1 if tr_len % batch_size else 0),\n","    validation_data=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n","    validation_steps=te_len // batch_size + (1 if te_len % batch_size else 0),\n","    # batch_size=batch_size,\n","    epochs=epochs,\n","    callbacks=[ModelCheckpoint(filepath=model_dir, \n","                               save_weights_only=True, \n","                               save_best_only=True)]\n",")\n","\n","# After training, load the best weights back\n","model_mf_dnn.load_weights(model_dir)\n","\n","sns.lineplot(np.arange(len(hist.history['loss'])), hist.history['loss'], label='train')\n","sns.lineplot(np.arange(len(hist.history['val_loss'])), hist.history['val_loss'], label='test')\n","plt.title('loss')\n","plt.grid(True)\n","plt.show()\n","\n","# Prediction\n","te_len = len(teProcessed)\n","pred = model_mf_dnn.predict_generator(\n","    generator=dataFn(teProcessed, n_batch=batch_size, shuffle=False)(),\n","    steps=te_len // batch_size + (1 if te_len % batch_size else 0)\n",").ravel()\n","\n","\n","te_labels = (teProcessed.rating >= 4).astype(int)\n","# AUC\n","print('Shape of test data: ', pred.shape)\n","draw_roc_curve(te_labels, pred)\n","\n","# Confusion matrix, classification report\n","print('accuracy_score: ', accuracy_score(te_labels, pred >= 0.5))\n","print(confusion_matrix(te_labels, pred >= 0.5))\n","print()\n","print(classification_report(te_labels, pred >= 0.5))\n","\n","most_like(model_emb_movie, 0, k=11)"],"execution_count":0,"outputs":[]}]}