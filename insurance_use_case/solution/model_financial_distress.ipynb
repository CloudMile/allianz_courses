{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6TzudIkLSb2"
   },
   "source": [
    "## Load Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3826,
     "status": "ok",
     "timestamp": 1538625947853,
     "user": {
      "displayName": "Gary Chen",
      "photoUrl": "",
      "userId": "10783188122132859162"
     },
     "user_tz": -480
    },
    "id": "_OceWebGEClz",
    "outputId": "979cfb33-6793-4122-a947-73d1bd147a91"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install seaborn --upgrade\n",
    "\n",
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, cv2\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, LabelBinarizer\n",
    "from sklearn.metrics import auc, roc_curve, f1_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, BatchNormalization, Activation, Input, Dropout, Embedding, Flatten, Input\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras import regularizers\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_result(hist):\n",
    "    plt.figure(figsize=(16, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(hist.history['loss'], label='tr_loss')\n",
    "    plt.plot(hist.history['val_loss'], label='vl_loss')\n",
    "    plt.title('Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(hist.history['acc'], label='acc')\n",
    "    plt.plot(hist.history['val_acc'], label='val_acc')\n",
    "    plt.title('Accuracy')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "def draw_roc_curve(y, pred):\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    fprRf, tprRf, _ = roc_curve(y, pred, pos_label=1)\n",
    "    auc_scr = auc(fprRf, tprRf)\n",
    "    print(\"auc:\", auc_scr)\n",
    "    f, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.plot(fprRf, tprRf, label='ROC CURVE')\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.set_title('Area Under Curve(ROC) (score: {:.4f})'.format(auc_scr))\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeEHBniStlgU"
   },
   "source": [
    "## Data Schema\n",
    "\n",
    "Column name     | Description\n",
    "             ---|---\n",
    "RorNumber       |    Row number\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ps1HdA6twJck"
   },
   "source": [
    "# Purpose: \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1451,
     "status": "ok",
     "timestamp": 1538625951034,
     "user": {
      "displayName": "Gary Chen",
      "photoUrl": "",
      "userId": "10783188122132859162"
     },
     "user_tz": -480
    },
    "id": "_UOyp6tUECl1",
    "outputId": "6b14c68e-7fb2-4959-dfec-8d970505dc52"
   },
   "outputs": [],
   "source": [
    "# raw = pd.read_csv(\"https://storage.googleapis.com/allianz-course/data/raw_tr_financial_churn.csv\")\n",
    "# raw_vl = pd.read_csv(\"https://storage.googleapis.com/allianz-course/data/raw_vl_financial_churn.csv\")\n",
    "# raw.head()\n",
    "raw = pd.read_csv('../data/raw_financial_distess.csv').rename(columns={'Financial Distress': 'distress_num'})\n",
    "raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1538625952553,
     "user": {
      "displayName": "Gary Chen",
      "photoUrl": "",
      "userId": "10783188122132859162"
     },
     "user_tz": -480
    },
    "id": "xbmIazS-_S9Z",
    "outputId": "61861644-02ea-4079-ab13-7f9c3b329469",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xoyVZct4tpFa"
   },
   "source": [
    "## Features setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "plpAXiiatNgD"
   },
   "outputs": [],
   "source": [
    "catg_features = ['x80']\n",
    "num_features = [f'x{i}' for i in range(1, 84) if i != 80]\n",
    "# embedding_features = catg_features + ['binn_tenure', 'binn_MonthlyCharges', 'binn_TotalCharges']\n",
    "label = 'distress_num'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GchbLNAmQaJo"
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rolling mean\n",
    "raw['rolling_mean'] = (raw.groupby('Company', group_keys=False)\n",
    "                          .apply(lambda pipe: pipe['distress_num'].rolling(window=3).mean().fillna(0))\n",
    "                          .values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train valid\n",
    "def split_fn(pipe):\n",
    "    lens = len(pipe)\n",
    "    ret = np.ones(lens)\n",
    "    if lens >= 5:\n",
    "        split_size = int(lens * 0.36)\n",
    "        ret[-split_size:] = 0\n",
    "    return ret\n",
    "\n",
    "raw['is_train'] = np.concatenate(raw.groupby('Company').apply(split_fn).values)\n",
    "raw_vl = raw.query(\"is_train == 0\").drop('is_train', 1)\n",
    "raw = raw.query(\"is_train == 1\").drop('is_train', 1)\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = raw.corr()\n",
    "corr['distress_num'][(corr['distress_num'] <= -0.3) | (corr['distress_num'] >= 0.3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop `Company`, Add `distress_catg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical label, not for training of course, for target mean encoding\n",
    "raw['distress_catg'] = (raw.distress_num <= 0.5).astype(int)\n",
    "# \n",
    "raw.drop(['Company'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCud9-1Esa6S"
   },
   "source": [
    "## Base Feature Preprocessing\n",
    "\n",
    "* Normalize ofr numeric features\n",
    "* One Hot Encoding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(raw, is_train=True, status=None):\n",
    "    data = raw.copy()\n",
    "    \n",
    "    if is_train:\n",
    "        status = {'scaler': None, \n",
    "                  'mapper': defaultdict(LabelBinarizer),\n",
    "                  'woe_mapper': {},\n",
    "                  'binn_mapper': {},\n",
    "                  'freq_mapper': {},\n",
    "                  'mean_mapper': {}}\n",
    "\n",
    "    catg_ftrs = catg_features.copy()\n",
    "        \n",
    "    # Numeric binning to categorical features\n",
    "    do_binning(catg_ftrs, status, data, is_train)\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "\n",
    "    catg_part = pd.DataFrame()\n",
    "    \n",
    "    # One Hot Encoding\n",
    "    # catg_part = pd.get_dummies(data[catg_ftrs])\n",
    "    # do_onehot(catg_ftrs, catg_part, status, data, is_train)\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "\n",
    "    # Embedding, simple indexing categorical features, actual embedding function in keras\n",
    "    do_embedding(catg_ftrs, catg_part, status, data, is_train)\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "\n",
    "    # WOE encoding\n",
    "    do_woe_encoding(catg_ftrs, catg_part, status, data, is_train)\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "\n",
    "    # Frequency encoding, target mean encoding\n",
    "    do_target_encoding(catg_ftrs, catg_part, status, data, is_train)\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "\n",
    "    # Normalize\n",
    "    num_part = do_norm(num_features, status, data, is_train)\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Merge categorical, numeric features\n",
    "    data_x, data_y = pd.concat([catg_part, num_part], 1), data['distress_num'].copy()\n",
    "    \n",
    "    # Include quadratic term, cube term\n",
    "    do_nth_order_polynominal(num_features, data_x)\n",
    "    # ----------------------------------------------------------------------------------------- \n",
    "    return data_x, data_y, status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of Features Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_binning(catg_ftrs, status, data, is_train):\n",
    "    pass\n",
    "\n",
    "def do_onehot(catg_ftrs, catg_part, status, data, is_train):\n",
    "    mapper = status['mapper']\n",
    "    tmp = []\n",
    "    for catg_col in catg_ftrs:\n",
    "        if is_train:\n",
    "            result = mapper[catg_col].fit_transform(data[catg_col])\n",
    "        else:\n",
    "            result = mapper[catg_col].transform(data[catg_col])\n",
    "\n",
    "        columns = [f'{catg_col}_{col}' for col in mapper[catg_col].classes_]\n",
    "        if result.shape[1] == 1:\n",
    "            columns = columns[:1]\n",
    "        tmp.append(pd.DataFrame(data=result, columns=columns))\n",
    "    tmp = pd.concat(tmp, 1)\n",
    "    for col in tmp:\n",
    "        catg_part[col] = tmp[col]\n",
    "    pass\n",
    "\n",
    "def do_woe_encoding(catg_ftrs, catg_part, status, data, is_train):\n",
    "    pass\n",
    "\n",
    "def do_target_encoding(catg_ftrs, catg_part, status, data, is_train):\n",
    "    pass\n",
    "\n",
    "def do_norm(num_features, status, data, is_train):\n",
    "    num_part = data[num_features].copy()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        status['scaler'] = scaler\n",
    "        num_part = pd.DataFrame(data=scaler.fit_transform(num_part), columns=num_part.columns)\n",
    "    else:\n",
    "        scaler = status['scaler']\n",
    "        num_part = pd.DataFrame(data=scaler.transform(num_part), columns=num_part.columns)\n",
    "    return num_part\n",
    "    pass\n",
    "\n",
    "def do_nth_order_polynominal(num_features, data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1538622762380,
     "user": {
      "displayName": "Gary Chen",
      "photoUrl": "",
      "userId": "10783188122132859162"
     },
     "user_tz": -480
    },
    "id": "udNVad7usaNe",
    "outputId": "22eba0e1-e10a-4822-d26a-af193fdf0362",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trnasform raw data to foramt model recognized\n",
    "tr_x, tr_y, status = feature_engineering(raw, is_train=True)\n",
    "vl_x, vl_y, _ = feature_engineering(raw_vl, is_train=False, status=status)\n",
    "\n",
    "print( tr_x.shape, vl_x.shape )\n",
    "tr_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52CvQZa1t2A7"
   },
   "source": [
    "## Model on Base Featue Engineering\n",
    "\n",
    "* Network Structure: \n",
    "    * `dimension of input, 64, 32, 16, 1`\n",
    "* ReLU Activation Function\n",
    "* Adam Optimizer\n",
    "* Sigmoid Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim):\n",
    "    model = Sequential()\n",
    "    # kernel_regularizer=regularizers.l2(0.005)\n",
    "    model.add(Dense(units = 6, activation = 'relu', kernel_initializer='uniform', input_dim = input_dim))\n",
    "    model.add(Dense(units = 6, activation = 'relu', kernel_initializer='uniform'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer='uniform'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1575
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4037,
     "status": "ok",
     "timestamp": 1538621868067,
     "user": {
      "displayName": "Gary Chen",
      "photoUrl": "",
      "userId": "10783188122132859162"
     },
     "user_tz": -480
    },
    "id": "SMJbH52ot1aD",
    "outputId": "70f5e5a7-4dc3-4269-f928-8c97c08f1598",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = get_model(input_dim=tr_x.shape[1])\n",
    "hist = model.fit(tr_x, tr_y, validation_data=(vl_x, vl_y), batch_size=100, epochs=30)\n",
    "plot_result(hist)\n",
    "draw_roc_curve(vl_y, model.predict(vl_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(vl_x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pred_dist(y_pred):\n",
    "    y_pred = pd.Series(y_pred.ravel())\n",
    "    print( y_pred.describe() )\n",
    "    print( y_pred.mean(), y_pred.std() )\n",
    "    sns.lineplot(range(len(y_pred)), sorted(y_pred.ravel()))\n",
    "    plt.show()\n",
    "\n",
    "    sns.boxplot(y_pred.ravel(), orient='v')\n",
    "    plt.show()\n",
    "\n",
    "    sns.distplot(y_pred.ravel())\n",
    "    plt.show()\n",
    "    \n",
    "pred_dist(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "* 預測機率範圍集中在 `0.273`左右, 已經是小數點3位以下的差距\n",
    "* Accuracy預設threshold = 0.5, model不會產出Positive的預測\n",
    "* 問題變成找出最佳Threshold ==> 預測機率大於多少時認為是`Exited = 1`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 找出最佳閥值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 依照所有的閥值(切割100等分)算出F score, 找出分數最高的閥值\n",
    "def f_beta_scann(y_true, y_pred, beta=0.5):\n",
    "    y_pred = pd.Series(y_pred.ravel())\n",
    "    # 切割100等分, 尋找最佳 f beta score\n",
    "    bins = np.linspace(y_pred.min(), y_pred.max(), 100)\n",
    "    # 找出F beta score最高的點\n",
    "    result = np.array([precision_recall_fscore_support(y_true=y_true, y_pred=y_pred > thres, beta=beta)[2][1] \n",
    "                       for thres in bins])\n",
    "    best_idx = result.argmax()\n",
    "    return bins[best_idx], result[best_idx]\n",
    "\n",
    "def analyze(y_true, y_pred):\n",
    "    thres, f_beta = f_beta_scann(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred.ravel() > thres)\n",
    "    print(f'thres: {thres:.4f}, f_beta score: {f_beta:.4f}')\n",
    "    print(f'accuracy@{thres:.4f}: {acc:.2f}\\n')\n",
    "\n",
    "    print(f'threshold@0.5, accuracy: {accuracy_score(y_true, y_pred.ravel() > 0.5):.4f}')\n",
    "    print(confusion_matrix(y_true, y_pred.ravel() > 0.5), '\\n')\n",
    "\n",
    "    print(f'threshold@{thres:.4f}, accuracy: {accuracy_score(y_true, y_pred.ravel() > thres):.4f}')\n",
    "    print(confusion_matrix(y_true, y_pred.ravel() > thres))\n",
    "    \n",
    "analyze(vl_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "* 整體Accuracy 0.75, 卻無法產出 `Exited = 1` 的預測\n",
    "* 經果F beta score掃描之後, 整體Accuracy 降低, 但能夠產出退租名單\n",
    "* 根據以上, 是否還會糾結於 Accuracy 的高低?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9MWinD-e6SX"
   },
   "source": [
    "## Bining Numeric Feature\n",
    "\n",
    "* Quartile cut\n",
    "* `Age`欄位特殊處理 => [14, 30, 35, 40, 45, 62]\n",
    "* `Balance`欄位特殊處理 => `IsBalanceZero`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1538632948251,
     "user": {
      "displayName": "Gary Chen",
      "photoUrl": "",
      "userId": "10783188122132859162"
     },
     "user_tz": -480
    },
    "id": "KEZCNmIMqlor",
    "outputId": "e3f26c9d-4262-4591-f24a-84e80ec60012"
   },
   "outputs": [],
   "source": [
    "def do_binning(catg_ftrs, status, data, is_train):\n",
    "    # Quartile binning, outlier set to class \"0\"\n",
    "    def quartile_binning(x):\n",
    "        bins = np.percentile(x, range(0, 100, 25))[1:].tolist()\n",
    "        iqr_x_150 = (bins[-1] - bins[0]) * 1.5\n",
    "        bins = [bins[0] - iqr_x_150] + bins + [bins[-1] + iqr_x_150]\n",
    "        result = pd.Series(np.digitize(x, bins)).map(pd.Series([0, 1, 2, 3, 4, 0])).values\n",
    "        return result, bins\n",
    "    \n",
    "    # Age\n",
    "    bins = np.array([14, 30, 35, 40, 45, 62])\n",
    "    labels = ['~14', '14-30', '30-35', '35-40', '40-45', '45-62', '62up']\n",
    "    age_map = pd.Series(labels)\n",
    "    data['binn_Age'] = pd.Series(np.digitize(data.Age, bins)).map(age_map).values\n",
    "    catg_ftrs.append('binn_Age')\n",
    "    # Balance\n",
    "    data['binn_Balance'] = data.Balance.map(lambda e: 0 if e == 0 else 1)\n",
    "    catg_ftrs.append('binn_Balance')\n",
    "    \n",
    "    for col in ('CreditScore', 'Tenure', 'EstimatedSalary'):\n",
    "        binned_name = f'binn_{col}'\n",
    "        if is_train:\n",
    "            result, bins = quartile_binning(data[col])\n",
    "            status['binn_mapper'][binned_name] = bins\n",
    "            data[binned_name] = result\n",
    "        else:\n",
    "            bins = status['binn_mapper'][binned_name]\n",
    "            data[binned_name] = pd.Series(np.digitize(data[col], bins))\\\n",
    "                                  .map(pd.Series([0, 1, 2, 3, 4, 0])).values\n",
    "\n",
    "        catg_ftrs.append(binned_name)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trnasform raw data to foramt model recognized\n",
    "tr_x, tr_y, status = feature_engineering(raw, is_train=True)\n",
    "vl_x, vl_y, _ = feature_engineering(raw_vl, is_train=False, status=status)\n",
    "\n",
    "print( tr_x.shape, vl_x.shape )\n",
    "tr_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (Binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim):\n",
    "    model = Sequential()\n",
    "    # kernel_regularizer=regularizers.l2(0.005)\n",
    "    model.add(Dense(units = 16, activation = 'relu', kernel_initializer='uniform', input_dim = input_dim))\n",
    "    model.add(Dense(units = 16, activation = 'relu', kernel_initializer='uniform'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer='uniform'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = get_model(input_dim=tr_x.shape[1])\n",
    "hist = model.fit(tr_x, tr_y, validation_data=(vl_x, vl_y), batch_size=100, epochs=30)\n",
    "plot_result(hist)\n",
    "draw_roc_curve(vl_y, model.predict(vl_x))\n",
    "\n",
    "y_pred = model.predict(vl_x)\n",
    "pred_dist(y_pred)\n",
    "analyze(vl_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EtiRvZDGVfV"
   },
   "source": [
    "## Add Weight of Evidence Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1538629657376,
     "user": {
      "displayName": "Gary Chen",
      "photoUrl": "",
      "userId": "10783188122132859162"
     },
     "user_tz": -480
    },
    "id": "L_YwlDOpGVml",
    "outputId": "abfe5e13-576a-4d23-acc6-9cc8f4db9c78"
   },
   "outputs": [],
   "source": [
    "def do_woe_encoding(catg_ftrs, catg_part, status, data, is_train):\n",
    "\n",
    "    def woe_encode(x, label, data):\n",
    "        \"\"\"Calculate the Weight of Evidence of given categorical feature and label\n",
    "\n",
    "        :param x: Given feature name\n",
    "        :param label: Label name\n",
    "        :param data:\n",
    "        :return: WOE encoded dictionary\n",
    "        \"\"\"\n",
    "        total_vc = data[label].value_counts().sort_index()\n",
    "\n",
    "        def woe(pipe, total_vc):\n",
    "            # Count by label in this group\n",
    "            group_vc = pipe[label].value_counts().sort_index()\n",
    "\n",
    "            # Some class in the feature is missing, fill zero to missing class\n",
    "            if len(group_vc) < len(total_vc):\n",
    "                for key in total_vc.index:\n",
    "                    if key not in group_vc:\n",
    "                        group_vc[key] = 0.\n",
    "                group_vc = group_vc.sort_index()\n",
    "\n",
    "            # WOE formula\n",
    "            r = ((group_vc + 0.5) / total_vc).values\n",
    "\n",
    "            # Odd ratio => 1 to 0, you can define meaning of each class\n",
    "            return np.log(r[1] / r[0])\n",
    "\n",
    "        return data.groupby(x).apply(lambda pipe: woe(pipe, total_vc))\n",
    "\n",
    "    for catg_col in catg_ftrs:\n",
    "        if is_train:\n",
    "            kv = woe_encode(catg_col, 'Exited', data)\n",
    "            status['woe_mapper'][catg_col] = kv.to_dict()\n",
    "        else:\n",
    "            kv = pd.Series(status['woe_mapper'][catg_col])\n",
    "        catg_part[f'woe_{catg_col}'] = kv.reindex(data[catg_col]).values\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trnasform raw data to foramt model recognized\n",
    "tr_x, tr_y, status = feature_engineering(raw, is_train=True)\n",
    "vl_x, vl_y, _ = feature_engineering(raw_vl, is_train=False, status=status)\n",
    "\n",
    "print( tr_x.shape, vl_x.shape )\n",
    "tr_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xp-VkesMgDVz"
   },
   "source": [
    "## Model (WOE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1609
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4738,
     "status": "ok",
     "timestamp": 1538629717958,
     "user": {
      "displayName": "Gary Chen",
      "photoUrl": "",
      "userId": "10783188122132859162"
     },
     "user_tz": -480
    },
    "id": "_1oC02RWeZoH",
    "outputId": "2c19cff1-1626-4319-8904-bf0f5bf81d36",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim):\n",
    "    model = Sequential()\n",
    "    # kernel_regularizer=regularizers.l2(0.005)\n",
    "    model.add(Dense(units = 20, activation = 'relu', kernel_initializer='uniform', input_dim = input_dim))\n",
    "    model.add(Dense(units = 20, activation = 'relu', kernel_initializer='uniform'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer='uniform'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = get_model(input_dim=tr_x.shape[1])\n",
    "hist = model.fit(tr_x, tr_y, validation_data=(vl_x, vl_y), batch_size=100, epochs=30)\n",
    "plot_result(hist)\n",
    "draw_roc_curve(vl_y, model.predict(vl_x))\n",
    "\n",
    "y_pred = model.predict(vl_x)\n",
    "pred_dist(y_pred)\n",
    "analyze(vl_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Encoding, Target Encoding\n",
    "\n",
    "* Frequency encoding\n",
    "* Mean encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_target_encoding(catg_ftrs, catg_part, status, data, is_train):\n",
    "    for catg_col in catg_ftrs:\n",
    "        if is_train:\n",
    "            freq_proportion = data[catg_col].value_counts() / len(data)\n",
    "            catg_part[f'freq_{catg_col}'] = freq_proportion.reindex(data[catg_col]).values\n",
    "            target_mean = data.groupby(catg_col).Exited.mean()\n",
    "            catg_part[f'mean_{catg_col}'] = target_mean.reindex(data[catg_col]).values\n",
    "\n",
    "            status['freq_mapper'][catg_col] = freq_proportion.to_dict()\n",
    "            status['mean_mapper'][catg_col] = target_mean.to_dict()\n",
    "        else:\n",
    "            catg_part[f'freq_{catg_col}'] = pd.Series(status['freq_mapper'][catg_col]).reindex(data[catg_col]).values\n",
    "            catg_part[f'mean_{catg_col}'] = pd.Series(status['mean_mapper'][catg_col]).reindex(data[catg_col]).values\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trnasform raw data to foramt model recognized\n",
    "tr_x, tr_y, status = feature_engineering(raw, is_train=True)\n",
    "vl_x, vl_y, _ = feature_engineering(raw_vl, is_train=False, status=status)\n",
    "\n",
    "print( tr_x.shape, vl_x.shape )\n",
    "tr_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (WOE + Binning + Freqency Encoding + Target Mean Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim):\n",
    "    model = Sequential()\n",
    "    # kernel_regularizer=regularizers.l2(0.005)\n",
    "    model.add(Dense(units = 30, activation = 'relu', kernel_initializer='uniform', input_dim = input_dim))\n",
    "    model.add(Dense(units = 30, activation = 'relu', kernel_initializer='uniform'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer='uniform'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = get_model(input_dim=tr_x.shape[1])\n",
    "hist = model.fit(tr_x, tr_y, validation_data=(vl_x, vl_y), batch_size=100, epochs=30)\n",
    "plot_result(hist)\n",
    "draw_roc_curve(vl_y, model.predict(vl_x))\n",
    "\n",
    "y_pred = model.predict(vl_x)\n",
    "pred_dist(y_pred)\n",
    "analyze(vl_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5crE3xFBn91"
   },
   "source": [
    "## Add Polynomial Featue Engineering\n",
    "\n",
    "* Add quradratic, cube term base on numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1538626022744,
     "user": {
      "displayName": "Gary Chen",
      "photoUrl": "",
      "userId": "10783188122132859162"
     },
     "user_tz": -480
    },
    "id": "0m8uAkjbuBa7",
    "outputId": "c318f7e6-0352-4a2a-b340-41b3b38e568a"
   },
   "outputs": [],
   "source": [
    "def do_nth_order_polynominal(num_features, data):\n",
    "    for num_col in num_features:\n",
    "        data[f'{num_col}_degree_2'] = data[num_col] ** 2\n",
    "        data[f'{num_col}_degree_3'] = data[num_col] ** 3\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trnasform raw data to foramt model recognized\n",
    "tr_x, tr_y, status = feature_engineering(raw, is_train=True)\n",
    "vl_x, vl_y, _ = feature_engineering(raw_vl, is_train=False, status=status)\n",
    "\n",
    "print( tr_x.shape, vl_x.shape )\n",
    "tr_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wfc1V2cYFz50"
   },
   "source": [
    "## Model (WOE + Binning + Freqency Encoding + Target Mean Encoding + Polynominal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1609
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4788,
     "status": "ok",
     "timestamp": 1538626029956,
     "user": {
      "displayName": "Gary Chen",
      "photoUrl": "",
      "userId": "10783188122132859162"
     },
     "user_tz": -480
    },
    "id": "Dl32_fhCFz-_",
    "outputId": "4cc0a562-f5c6-44fd-ce29-1c52974296f2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim):\n",
    "    model = Sequential()\n",
    "    # kernel_regularizer=regularizers.l2(0.005)\n",
    "    model.add(Dense(units = 35, activation = 'relu', kernel_initializer='uniform', input_dim = input_dim))\n",
    "    model.add(Dense(units = 35, activation = 'relu', kernel_initializer='uniform'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer='uniform'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = get_model(input_dim=tr_x.shape[1])\n",
    "hist = model.fit(tr_x, tr_y, validation_data=(vl_x, vl_y), batch_size=100, epochs=30)\n",
    "plot_result(hist)\n",
    "draw_roc_curve(vl_y, model.predict(vl_x))\n",
    "\n",
    "y_pred = model.predict(vl_x)\n",
    "pred_dist(y_pred)\n",
    "analyze(vl_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=64)\n",
    "tr_x_pca = pca.fit_transform(tr_x)\n",
    "vl_x_pca = pca.transform(vl_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x_pca.shape, vl_x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim):\n",
    "    model = Sequential()\n",
    "    # kernel_regularizer=regularizers.l2(0.005)\n",
    "    model.add(Dense(units = 32, activation = 'relu', kernel_initializer='uniform', input_dim = input_dim))\n",
    "    model.add(Dense(units = 32, activation = 'relu', kernel_initializer='uniform'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer='uniform'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = get_model(input_dim=tr_x_pca.shape[1])\n",
    "hist = model.fit(tr_x_pca, tr_y, validation_data=(vl_x_pca, vl_y), batch_size=100, epochs=30)\n",
    "plot_result(hist)\n",
    "draw_roc_curve(vl_y, model.predict(vl_x_pca))\n",
    "\n",
    "y_pred = model.predict(vl_x_pca)\n",
    "pred_dist(y_pred)\n",
    "analyze(vl_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (AutoEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trnasform raw data to foramt model recognized\n",
    "tr_x, tr_y, status = feature_engineering(raw, is_train=True)\n",
    "vl_x, vl_y, _ = feature_engineering(raw_vl, is_train=False, status=status)\n",
    "\n",
    "print( tr_x.shape, vl_x.shape )\n",
    "tr_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "inputs_dim = tr_x.shape[1]\n",
    "\n",
    "inputs = Input(shape=(inputs_dim, ))\n",
    "# Encoder\n",
    "encoded = Dense(inputs_dim, activation='selu')(inputs)\n",
    "encoded = Dense(128, activation='selu')(encoded)\n",
    "encoded = Dense(64, activation='selu')(encoded)\n",
    "encoded = Dense(64, activation='selu')(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(64, activation='selu')(encoded)\n",
    "decoded = Dense(64, activation='selu')(decoded)\n",
    "decoded = Dense(128, activation='selu')(decoded)\n",
    "decoded = Dense(inputs_dim, activation='linear')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(inputs, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "# Adam Optimizer + Mean square error loss\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "hist = autoencoder.fit(\n",
    "    tr_x, tr_x, \n",
    "    epochs=100, \n",
    "    batch_size=200, \n",
    "    shuffle=True, \n",
    "    validation_data=(vl_x, vl_x))\n",
    "\n",
    "def plot_ae(hist):\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.plot(hist.history['loss'], label='tr_loss')\n",
    "    plt.plot(hist.history['val_loss'], label='vl_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "plot_ae(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_x_ae = encoder.predict(tr_x)\n",
    "vl_x_ae = encoder.predict(vl_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x_ae.shape, vl_x_ae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_dim):\n",
    "    model = Sequential()\n",
    "    # kernel_regularizer=regularizers.l2(0.005)\n",
    "    model.add(Dense(units = 128, activation = 'relu', kernel_initializer='uniform', input_dim = input_dim))\n",
    "    model.add(Dense(units = 128, activation = 'relu', kernel_initializer='uniform'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer='uniform'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = get_model(input_dim=tr_x_ae.shape[1])\n",
    "hist = model.fit(tr_x_ae, tr_y, validation_data=(vl_x_ae, vl_y), batch_size=100, epochs=30)\n",
    "\n",
    "plot_result(hist)\n",
    "draw_roc_curve(vl_y, model.predict(vl_x_ae))\n",
    "\n",
    "y_pred = model.predict(vl_x_ae)\n",
    "pred_dist(y_pred)\n",
    "analyze(vl_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_finance_customer_churn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
